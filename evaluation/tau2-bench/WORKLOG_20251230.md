# Tau2-Bench OSS Evaluation Worklog

**Date**: 2025-12-30

---

## 1. Experiment Overview

Ran tau2-bench evaluation with OSS expert models to benchmark the Orchestrator-8B agent with 3 expert LLMs.

### Model Configuration

| Model | Role | Node | GPUs | Ports | Config |
|-------|------|------|------|-------|--------|
| Nemotron-Orchestrator-8B | Agent | Node1 | 0-3 | 1900-1903 | TP=1, DP=4 |
| Qwen/Qwen3-32B-FP8 | Expert-2 | Node1 | 4-7 | 1904-1907 | TP=1, DP=4, non-thinking |
| openai/gpt-oss-20b | Expert-1 | Node2 | 0-3 | 1910-1913 | TP=1, DP=4 |
| Qwen/Qwen3-Next-80B-A3B-Instruct-FP8 | Expert-3 | Node3 | 0-7 | 1920-1923 | TP=2, DP=4, MTP 4 tokens |

---

## 2. How Experiments Were Launched

### Step 1: Deploy vLLM Servers (3 SLURM jobs)

```bash
cd /home/junxiong/haokang/ToolOrchestra/evaluation/tau2-bench

# Submit Node1: Orchestrator-8B + Qwen3-32B-FP8
sbatch oss_eval_node1.sh

# Submit Node2: gpt-oss-20b
sbatch oss_eval_node2.sh

# Submit Node3: Qwen3-Next-80B-A3B-Instruct-FP8
sbatch oss_eval_node3.sh

# Wait for servers to start (~10 minutes)
squeue --me
```

### Step 2: Verify Endpoints

```bash
# Get node IPs from output files
head -1 oss_eval_node1.out  # -> 172.27.25.244
head -1 oss_eval_node2.out  # -> 172.27.20.172
head -1 oss_eval_node3.out  # -> 172.27.25.31

# Test endpoints
for port in 1900 1901 1902 1903; do
  curl -s "http://172.27.25.244:${port}/v1/models"
done
```

### Step 3: Update model_config_oss.json

Updated with actual node IPs after SLURM allocation.

### Step 4: Run Smoke Test (mock domain)

```bash
python -m tau2.cli \
    --domain mock \
    --agent-llm $CKPT_DIR \
    --user-llm gpt-5 \
    --task_path /home/junxiong/haokang/ToolOrchestra/data/tau2/domains/mock/tasks.json \
    --output_file smoke_test_oss/mock.json \
    --model_config_path model_config_oss.json \
    --max-concurrency 4 \
    --use_model_tool \
    --log-level DEBUG \
    2>&1 | tee tmp_logs/smoke_mock.log
```

### Step 5: Run Full Evaluation

```bash
python run_oss.py \
    --domains retail telecom airline \
    --skip-server-start \
    --log-level PROFILE \
    --output-dir outputs_oss_full \
    --log-dir tmp_logs \
    2>&1 | tee tmp_logs/fullrun.log
```

---

## 3. Analysis Scripts and Output Paths

### Analysis Script

```bash
# Main analysis script
python analyze_profile.py tmp_logs/tau2_*.log
# to analyze the experiments we did today, use:
# python analyze_profile.py logs_oss_full/tau2_*.log 2>&1 

# Generates:
# - profile_analysis.json (aggregated stats)
# - profile_charts/*.png (latency distribution charts)
# - experiment_report_<timestamp>.md (markdown report)
```

### Output Paths

| File/Directory | Purpose |
|----------------|---------|
| `model_config_oss.json` | vLLM endpoint configuration |
| `tmp_logs/fullrun.log` | Main evaluation log |
| `tmp_logs/tau2_*.log` | Per-domain profiling logs |
| `outputs_oss_full/*.json` | Evaluation results (per domain) |
| `profile_analysis.json` | Aggregated latency statistics |
| `profile_charts/*.png` | Latency distribution visualizations |
| `experiment_report_20251230_194749.md` | Final markdown report |

---

## 4. Key Results

### Evaluation Pass Rates

| Domain | Tasks | Pass Rate |
|--------|-------|-----------|
| retail | 56 | 44.6% |
| telecom | 72 | 43.1% |
| airline | 34 | 32.4% |

### Model Latency (P50, ms)

| Model | P50 | <5s% |
|-------|-----|------|
| Orchestrator | 4020 | 62.9% |
| gpt-oss-20b | 1774 | 87.6% |
| Qwen3-32B-FP8 | 4593 | 56.7% |
| Qwen3-Next-80B | 1253 | 93.3% |
| user_sim:gpt-5 | 9608 | 29.1% |

---

## 5. Problems Encountered & Solutions

1. **`unexpected tokens remaining in message header`** (~3.4% error rate)
   - Cause: Orchestrator-8B outputting `<|channel|>commentary` tokens
   - Fix: Use `--chat-template tool_chat_template_llama3.1_json.jinja`

2. **CUDA OOM for Qwen3-Next-80B with MTP**
   - Fix: Reduce `gpu_memory_utilization` from 0.95 to 0.88

3. **Qwen3 thinking mode slowing down experts**
   - Fix: Add `/no_think` suffix to user messages

---

## 6. Files Modified

| File | Changes |
|------|---------|
| `run_oss.py` | Added ORCHESTRATOR_CHAT_TEMPLATE, GPU cleanup, 24h SLURM time |
| `tau2/utils/llm_utils.py` | Added streaming profiling (prefill_ms, decode_ms), Qwen3 non-thinking mode |
| `analyze_profile.py` | New script for latency analysis, PNG charts, markdown reports |
| `oss_eval_node1.sh` | Added --chat-template for Orchestrator-8B |
| `OSS_EVAL_NOTES.md` | Documentation of changes and best practices |

---

## 7. Next Steps

1. Re-run with fixed chat template to eliminate "unexpected tokens" errors
2. Investigate Qwen3-32B-FP8 slower latency (P50=4.6s vs gpt-oss-20b P50=1.8s)
3. Analyze airline domain's lower pass rate (32.4% vs ~44%)
