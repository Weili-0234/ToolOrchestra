# Tau2-Bench OSS Evaluation Worklog

**Date**: 2025-12-30

---

## 1. Experiment Overview

Ran tau2-bench evaluation with OSS expert models to benchmark the Orchestrator-8B agent with 3 expert LLMs.

### Model Configuration

| Model | Role | Node | GPUs | Ports | Config |
|-------|------|------|------|-------|--------|
| Nemotron-Orchestrator-8B | Agent | Node1 | 0-3 | 1900-1903 | TP=1, DP=4 |
| Qwen/Qwen3-32B-FP8 | Expert-2 | Node1 | 4-7 | 1904-1907 | TP=1, DP=4, non-thinking |
| openai/gpt-oss-20b | Expert-1 | Node2 | 0-3 | 1910-1913 | TP=1, DP=4 |
| Qwen/Qwen3-Next-80B-A3B-Instruct-FP8 | Expert-3 | Node3 | 0-7 | 1920-1923 | TP=2, DP=4, MTP 4 tokens |

---

## 2. How Experiments Were Launched

### Step 1: Deploy vLLM Servers (3 SLURM jobs)

```bash
cd /home/junxiong/haokang/ToolOrchestra/evaluation/tau2-bench

# Submit Node1: Orchestrator-8B + Qwen3-32B-FP8
sbatch oss_eval_node1.sh

# Submit Node2: gpt-oss-20b
sbatch oss_eval_node2.sh

# Submit Node3: Qwen3-Next-80B-A3B-Instruct-FP8
sbatch oss_eval_node3.sh

# Wait for servers to start (~10 minutes)
squeue --me
```

### Step 2: Verify Endpoints

```bash
# Get node IPs from output files
head -1 oss_eval_node1.out  # -> 172.27.25.244
head -1 oss_eval_node2.out  # -> 172.27.20.172
head -1 oss_eval_node3.out  # -> 172.27.25.31

# Test endpoints
for port in 1900 1901 1902 1903; do
  curl -s "http://172.27.25.244:${port}/v1/models"
done
```

### Step 3: Update model_config_oss.json

Updated with actual node IPs after SLURM allocation.

### Step 4: Run Smoke Test (mock domain)

```bash
python -m tau2.cli \
    --domain mock \
    --agent-llm $CKPT_DIR \
    --user-llm gpt-5 \
    --task_path /home/junxiong/haokang/ToolOrchestra/data/tau2/domains/mock/tasks.json \
    --output_file smoke_test_oss/mock.json \
    --model_config_path model_config_oss.json \
    --max-concurrency 4 \
    --use_model_tool \
    --log-level DEBUG \
    2>&1 | tee tmp_logs/smoke_mock.log
```

### Step 5: Run Full Evaluation

```bash
python run_oss.py \
    --domains retail telecom airline \
    --skip-server-start \
    --log-level PROFILE \
    --output-dir outputs_oss_full \
    --log-dir tmp_logs \
    2>&1 | tee tmp_logs/fullrun.log
# max_concurrency is 10 as by default
```

---

## 3. Log and Output Paths

### Today's Experiment Logs

| Path | Description |
|------|-------------|
| `logs_oss_full/fullrun.log` | Main evaluation log (9.4 MB) |
| `logs_oss_full/tau2_retail.log` | Retail domain profiling log (1.1 MB) |
| `logs_oss_full/tau2_telecom.log` | Telecom domain profiling log (3.9 MB) |
| `logs_oss_full/tau2_airline.log` | Airline domain profiling log (466 KB) |
| `logs_oss_full/eval_retail.log` | Retail evaluation details (2.2 MB) |
| `logs_oss_full/eval_telecom.log` | Telecom evaluation details (6.1 MB) |
| `logs_oss_full/eval_airline.log` | Airline evaluation details (843 KB) |

### Evaluation Results

| Path | Description |
|------|-------------|
| `outputs_oss_full/retail/*.json` | Retail domain task results |
| `outputs_oss_full/telecom/*.json` | Telecom domain task results |
| `outputs_oss_full/airline/*.json` | Airline domain task results |

### Profile Analysis Outputs

| Path | Description |
|------|-------------|
| `profile_analysis.json` | Aggregated latency statistics (JSON) |
| `profile_charts/*.png` | Latency distribution visualizations |
| `experiment_report_20251230_*.md` | Detailed markdown report |

### Analysis Script Usage

```bash
# Analyze today's experiment logs:
cd /home/junxiong/haokang/ToolOrchestra/evaluation/tau2-bench
python analyze_profile.py logs_oss_full/tau2_*.log

# Outputs:
# - profile_analysis.json        (JSON stats with p10-p99)
# - profile_charts/*.png         (latency distribution charts)
# - experiment_report_<timestamp>.md  (markdown report)
```

**What the script computes:**
1. Per-model latency stats: count, mean, std, min, max, p10, p20, p25, p30, p40, p50, p60, p70, p80, p90, p95, p99, <5s%
2. Prefill/decode breakdown for vLLM streaming
3. Token throughput (tokens/sec)
4. 10-bin histogram
5. PNG charts (Histogram + CDF)

---

## 4. Key Results

### Evaluation Pass Rates

| Domain | Tasks | Pass Rate |
|--------|-------|-----------|
| retail | 56 | 44.6% |
| telecom | 72 | 43.1% |
| airline | 34 | 32.4% |

---

## 5. Detailed Model Latency Statistics (ms)

### 5.1 Orchestrator (Nemotron-Orchestrator-8B)

| Metric | Value |
|--------|-------|
| **Count** | 3453 |
| **Mean** | 6234.4 |
| **Std** | 13415.5 |
| **Min** | 234.1 |
| **Max** | 426958.6 |
| **P10** | 2096.5 |
| **P20** | 2601.1 |
| **P25** | 2804.8 |
| **P30** | 3029.6 |
| **P40** | 3468.5 |
| **P50** | 4020.2 |
| **P60** | 4752.6 |
| **P70** | 5702.0 |
| **P80** | 7169.6 |
| **P90** | 9700.0 |
| **P95** | 13301.1 |
| **P99** | 70584.2 |
| **<5s%** | 62.9% |

**10-Bin Distribution:**
```
Bin Range (ms)         Count  Histogram
234.1-42906.5           3410  ########################################
42906.5-85579.0           34
85579.0-128251.4           3
128251.4-170923.9          2
170923.9-213596.4          1
213596.4-256268.8          0
256268.8-298941.3          1
298941.3-341613.7          1
341613.7-384286.2          0
384286.2-426958.6          1
```

**Prefill/Decode Breakdown:**
| Phase | Mean | P50 | P90 | P95 |
|-------|------|-----|-----|-----|
| Prefill | 93.4 | 72.5 | 139.6 | 209.1 |
| Decode | 4246.1 | 3050.5 | 6348.0 | 8584.3 |

---

### 5.2 Expert-1: openai/gpt-oss-20b

| Metric | Value |
|--------|-------|
| **Count** | 963 |
| **Mean** | 4561.9 |
| **Std** | 21389.0 |
| **Min** | 217.1 |
| **Max** | 421373.5 |
| **P10** | 655.1 |
| **P20** | 886.7 |
| **P25** | 1015.1 |
| **P30** | 1145.6 |
| **P40** | 1462.4 |
| **P50** | 1774.2 |
| **P60** | 2328.7 |
| **P70** | 2921.0 |
| **P80** | 3932.6 |
| **P90** | 5576.3 |
| **P95** | 7525.7 |
| **P99** | 56669.4 |
| **<5s%** | 87.6% |

**10-Bin Distribution:**
```
Bin Range (ms)         Count  Histogram
217.1-42332.7            952  ########################################
42332.7-84448.4            2
84448.4-126564.0           3
126564.0-168679.6          2
168679.6-210795.3          1
210795.3-252910.9          0
252910.9-295026.6          1
295026.6-337142.2          1
337142.2-379257.9          0
379257.9-421373.5          1
```

**Prefill/Decode Breakdown:**
| Phase | Mean | P50 | P90 | P95 |
|-------|------|-----|-----|-----|
| Prefill | 1710.2 | 1305.1 | 3427.6 | 4429.0 |
| Decode | 620.5 | 227.3 | 1886.5 | 2522.3 |

---

### 5.3 Expert-2: Qwen/Qwen3-32B-FP8

| Metric | Value |
|--------|-------|
| **Count** | 402 |
| **Mean** | 5303.1 |
| **Std** | 3081.1 |
| **Min** | 1682.6 |
| **Max** | 34865.1 |
| **P10** | 2678.7 |
| **P20** | 3052.1 |
| **P25** | 3284.1 |
| **P30** | 3442.2 |
| **P40** | 4022.1 |
| **P50** | 4593.0 |
| **P60** | 5303.5 |
| **P70** | 6061.6 |
| **P80** | 6901.2 |
| **P90** | 8889.3 |
| **P95** | 10481.5 |
| **P99** | 13774.9 |
| **<5s%** | 56.7% |

**10-Bin Distribution:**
```
Bin Range (ms)         Count  Histogram
1682.6-5000.9            228  ########################################
5000.9-8319.1            126  ######################
8319.1-11637.4            35  ######
11637.4-14955.6           10  #
14955.6-18273.9            1
18273.9-21592.1            0
21592.1-24910.3            1
24910.3-28228.6            0
28228.6-31546.8            0
31546.8-34865.1            1
```

**Note:** No prefill/decode breakdown (Together.AI hosted model)

---

### 5.4 Expert-3: Qwen/Qwen3-Next-80B-A3B-Instruct-FP8

| Metric | Value |
|--------|-------|
| **Count** | 15 |
| **Mean** | 1691.7 |
| **Std** | 1575.6 |
| **Min** | 434.0 |
| **Max** | 6938.4 |
| **P10** | 579.9 |
| **P20** | 694.2 |
| **P25** | 778.3 |
| **P30** | 855.7 |
| **P40** | 927.5 |
| **P50** | 1252.8 |
| **P60** | 1541.0 |
| **P70** | 1712.4 |
| **P80** | 1915.1 |
| **P90** | 2839.3 |
| **P95** | 4267.4 |
| **P99** | 6404.2 |
| **<5s%** | 93.3% |

**10-Bin Distribution:**
```
Bin Range (ms)         Count  Histogram
434.0-1084.5               7  ########################################
1084.5-1734.9              4  #######################
1734.9-2385.3              1  #####
2385.3-3035.8              1  #####
3035.8-3686.2              1  #####
3686.2-4336.6              0
4336.6-4987.1              0
4987.1-5637.5              0
5637.5-6287.9              0
6287.9-6938.4              1  #####
```

**Prefill/Decode Breakdown:**
| Phase | Mean | P50 | P90 | P95 |
|-------|------|-----|-----|-----|
| Prefill | 505.3 | 349.0 | 895.9 | 1385.6 |
| Decode | 1157.2 | 556.6 | 1875.6 | 3474.3 |

---

### 5.5 User Simulator: gpt-5

| Metric | Value |
|--------|-------|
| **Count** | 2335 |
| **Mean** | 11663.3 |
| **Std** | 13695.2 |
| **Min** | 756.8 |
| **Max** | 497098.7 |
| **P10** | 1941.3 |
| **P20** | 2830.2 |
| **P25** | 3864.1 |
| **P30** | 5251.7 |
| **P40** | 7457.3 |
| **P50** | 9608.2 |
| **P60** | 11809.8 |
| **P70** | 14605.9 |
| **P80** | 17695.0 |
| **P90** | 23387.7 |
| **P95** | 29060.5 |
| **P99** | 43541.8 |
| **<5s%** | 29.1% |

**10-Bin Distribution:**
```
Bin Range (ms)         Count  Histogram
756.8-50390.9           2324  ########################################
50390.9-100025.1          10
100025.1-149659.3          0
149659.3-199293.5          0
199293.5-248927.7          0
248927.7-298561.9          0
298561.9-348196.1          0
348196.1-397830.3          0
397830.3-447464.5          0
447464.5-497098.7          1
```

---

## 6. Problems Encountered & Solutions

1. **`unexpected tokens remaining in message header`** (~3.4% error rate)
   - Cause: Orchestrator-8B outputting `<|channel|>commentary` tokens
   - Fix: Use `--chat-template tool_chat_template_llama3.1_json.jinja`

2. **CUDA OOM for Qwen3-Next-80B with MTP**
   - Fix: Reduce `gpu_memory_utilization` from 0.95 to 0.88

3. **Qwen3 thinking mode slowing down experts**
   - Fix: Add `/no_think` suffix to user messages

---

## 7. Files Modified

| File | Changes |
|------|---------|
| `run_oss.py` | Added ORCHESTRATOR_CHAT_TEMPLATE, GPU cleanup, 24h SLURM time |
| `tau2/utils/llm_utils.py` | Added streaming profiling (prefill_ms, decode_ms), Qwen3 non-thinking mode |
| `analyze_profile.py` | New script for latency analysis, PNG charts, markdown reports |
| `oss_eval_node1.sh` | Added --chat-template for Orchestrator-8B |
| `OSS_EVAL_NOTES.md` | Documentation of changes and best practices |

---

## 8. Next Steps

1. Re-run with fixed chat template to eliminate "unexpected tokens" errors
2. Investigate Qwen3-32B-FP8 slower latency (P50=4.6s vs gpt-oss-20b P50=1.8s)
3. Analyze airline domain's lower pass rate (32.4% vs ~44%)
