# Ï„Â²-Bench æœ¬åœ°è¿è¡ŒæŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•åœ¨æœ¬åœ°ç¯å¢ƒï¼ˆé SLURM é›†ç¾¤ï¼‰è¿è¡Œ Ï„Â²-Bench è¯„æµ‹ã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

### 1. Conda ç¯å¢ƒ

åˆ›å»ºå¹¶æ¿€æ´» `vllm1` ç¯å¢ƒï¼š

```bash
conda create -n vllm1 python=3.12 -y
conda activate vllm1
pip install torch
pip install "transformers<4.54.0"
pip install vllm==0.9.2
cd evaluation/tau2-bench
pip install -e .
```

æ­£åœ¨å°è¯•ä¸‹é¢è¿™ä¸ªèƒ½ä¸èƒ½work
```bash
conda activate vllm1 && pip uninstall -y vllm torch transformers && pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu128 && pip install vllm==0.10.1 transformers
pip install hf_transfer # vllm 10.1 å¾€åä¼šç”¨åˆ° hf_transfer
```

### 2. ç¯å¢ƒå˜é‡

è®¾ç½®ä»¥ä¸‹å¿…éœ€çš„ç¯å¢ƒå˜é‡ï¼ˆå¯ä»¥æ·»åŠ åˆ° `setup_envs.sh`ï¼‰ï¼š

```bash
# æ¨¡å‹å’Œæ•°æ®è·¯å¾„
export CKPT_DIR="/path/to/your/agent/model"     # Agent æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
export REPO_PATH="/path/to/ToolOrchestra"       # ä»“åº“æ ¹ç›®å½•
export HF_HOME="/path/to/huggingface"           # HuggingFace ç¼“å­˜ç›®å½•

# API Keys (ç”¨äº user simulation å’Œå¯èƒ½çš„ judge model)
export OPENAI_API_KEY="your_openai_api_key"     # ç”¨äº gpt-4o ä½œä¸º user-llm
export ANTHROPIC_API_KEY="your_anthropic_key"   # å¯é€‰ï¼Œå¦‚æœä½¿ç”¨ Claude
```

åŠ è½½ç¯å¢ƒå˜é‡ï¼š
```bash
source setup_envs.sh
```

### 3. GPU è¦æ±‚

- **æœ€å°‘**: 1 ä¸ª GPUï¼ˆåªå¯åŠ¨ 1 ä¸ª agent serverï¼‰
- **æ¨è**: 4+ ä¸ª GPUï¼ˆå¯ä»¥å¹¶è¡Œè¿è¡Œå¤šä¸ª agent serverï¼ŒåŠ å¿«è¯„æµ‹é€Ÿåº¦ï¼‰

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼ï¼š

```bash
cd evaluation/tau2-bench/
python run_local.py --agent-model $CKPT_DIR
```

è¿™å°†ä¼šï¼š
1. å¯åŠ¨ 4 ä¸ª vLLM agent æœåŠ¡å™¨ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿçš„ GPUï¼‰
2. ä¾æ¬¡è¯„æµ‹ retailã€telecomã€airline ä¸‰ä¸ªåŸŸ
3. ç»“æœä¿å­˜åœ¨ `outputs/` ç›®å½•

### å¸¸ç”¨é€‰é¡¹

#### æŒ‡å®šæœåŠ¡å™¨æ•°é‡
```bash
# åªå¯åŠ¨ 1 ä¸ªæœåŠ¡å™¨ï¼ˆé€‚åˆ GPU èµ„æºæœ‰é™çš„æƒ…å†µï¼‰
python run_local.py --agent-model $CKPT_DIR --num-servers 1

# å¯åŠ¨ 8 ä¸ªæœåŠ¡å™¨ï¼ˆå¦‚æœæœ‰ 8 ä¸ª GPUï¼‰
python run_local.py --agent-model $CKPT_DIR --num-servers 8
```

#### æŒ‡å®šè¦è¯„æµ‹çš„åŸŸ
```bash
# åªè¯„æµ‹ retail åŸŸ
python run_local.py --agent-model $CKPT_DIR --domains retail

# è¯„æµ‹ retail å’Œ telecom
python run_local.py --agent-model $CKPT_DIR --domains retail telecom
```

#### æŒ‡å®š user-llm
```bash
# ä½¿ç”¨ gpt-4oï¼ˆé»˜è®¤ï¼‰
python run_local.py --agent-model $CKPT_DIR --user-llm gpt-4o

# ä½¿ç”¨ Claude
python run_local.py --agent-model $CKPT_DIR --user-llm claude-3-5-sonnet-20241022
```

#### æŒ‡å®š GPU
```bash
# ä½¿ç”¨ç‰¹å®šçš„ GPU (ä¾‹å¦‚ GPU 2 å’Œ 3)
python run_local.py --agent-model $CKPT_DIR --num-servers 2 --gpu-ids 2 3
```

#### è°ƒæ•´å¯åŠ¨å‚æ•°
```bash
# å‡å°‘æœåŠ¡å™¨å¯åŠ¨é—´éš”ï¼ˆé»˜è®¤ 60 ç§’ï¼‰
python run_local.py --agent-model $CKPT_DIR --stagger-delay 30

# å¢åŠ æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤ 600 ç§’ï¼‰
python run_local.py --agent-model $CKPT_DIR --server-timeout 900
```

### é«˜çº§ç”¨æ³•

#### ä½¿ç”¨å·²ç»è¿è¡Œçš„æœåŠ¡å™¨

å¦‚æœä½ å·²ç»æ‰‹åŠ¨å¯åŠ¨äº† vLLM æœåŠ¡å™¨ï¼Œå¯ä»¥è·³è¿‡å¯åŠ¨æ­¥éª¤ï¼š

```bash
# å…ˆæ‰‹åŠ¨åˆ›å»ºé…ç½®æ–‡ä»¶ model_config_local.json
cat > model_config_local.json << EOF
{
  "/path/to/model": [
    {"ip_addr": "127.0.0.1", "port": "1900"},
    {"ip_addr": "127.0.0.1", "port": "1901"}
  ],
  "vllm_model_config_path": "model_config_local.json"
}
EOF

# ç„¶åè¿è¡Œè¯„æµ‹
python run_local.py --agent-model $CKPT_DIR --skip-server-start
```

#### å®Œæ•´ç¤ºä¾‹å‘½ä»¤

```bash
python run_local.py \
  --agent-model /data/models/Orchestrator-8B \
  --user-llm gpt-4o \
  --num-servers 4 \
  --domains retail telecom airline \
  --max-steps 200 \
  --num-trials 1 \
  --output-dir outputs/run1 \
  --log-dir logs/run1
```

## ğŸ“Š è¾“å‡ºè¯´æ˜

### æ—¥å¿—æ–‡ä»¶

- **æœåŠ¡å™¨æ—¥å¿—**: `logs/vllm_port_XXXX_*.out` å’Œ `*.err`
  - åŒ…å«æ¯ä¸ª vLLM æœåŠ¡å™¨çš„å¯åŠ¨æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯

### ç»“æœæ–‡ä»¶

- **è¯„æµ‹ç»“æœ**: `outputs/{domain}.json`
  - æ¯ä¸ªåŸŸçš„è¯¦ç»†è¯„æµ‹ç»“æœ

### é…ç½®æ–‡ä»¶

- **æ¨¡å‹é…ç½®**: `model_config_local.json`
  - è‡ªåŠ¨ç”Ÿæˆçš„æ¨¡å‹æœåŠ¡å™¨é…ç½®æ–‡ä»¶
  - tau2-bench ä½¿ç”¨æ­¤æ–‡ä»¶è¿æ¥åˆ° vLLM æœåŠ¡å™¨

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: GPU å†…å­˜ä¸è¶³

**é”™è¯¯**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
- å‡å°‘å¹¶è¡ŒæœåŠ¡å™¨æ•°é‡: `--num-servers 1`
- æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### é—®é¢˜ 2: æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶

**é”™è¯¯**: `Server on port XXXX failed to start within 600s`

**è§£å†³æ–¹æ¡ˆ**:
- å¢åŠ è¶…æ—¶æ—¶é—´: `--server-timeout 1200`
- æ£€æŸ¥ `logs/` ç›®å½•ä¸­çš„æ—¥å¿—æ–‡ä»¶æ‰¾å‡ºåŸå› 
- ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ä¸”å¯è®¿é—®

### é—®é¢˜ 3: REPO_PATH æœªè®¾ç½®

**é”™è¯¯**: `ERROR: Task file not found`

**è§£å†³æ–¹æ¡ˆ**:
```bash
export REPO_PATH="/path/to/ToolOrchestra"
```

### é—®é¢˜ 4: API Key æœªè®¾ç½®

**é”™è¯¯**: `API key not found` æˆ– `Authentication failed`

**è§£å†³æ–¹æ¡ˆ**:
```bash
export OPENAI_API_KEY="your_key_here"
# æˆ–
export ANTHROPIC_API_KEY="your_key_here"
```

### é—®é¢˜ 5: ç«¯å£è¢«å ç”¨

**é”™è¯¯**: `Address already in use`

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨ä¸åŒçš„èµ·å§‹ç«¯å£: `--start-port 2000`
- æˆ–åœæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹:
  ```bash
  lsof -ti:1900 | xargs kill -9
  ```

## ğŸ“ ä¸åŸç‰ˆ run.py çš„åŒºåˆ«

| ç‰¹æ€§ | run.py (SLURM) | run_local.py (æœ¬åœ°) |
|------|----------------|---------------------|
| è°ƒåº¦ç³»ç»Ÿ | SLURM | ç›´æ¥å¯åŠ¨è¿›ç¨‹ |
| æœåŠ¡å™¨ IP | é›†ç¾¤èŠ‚ç‚¹ IP | 127.0.0.1 (localhost) |
| Judge Model | å¯åŠ¨æœ¬åœ° Qwen3-32B | é€šè¿‡ API è°ƒç”¨ |
| GPU åˆ†é… | SLURM ç®¡ç† | æ‰‹åŠ¨æŒ‡å®š GPU ID |
| æ—¥å¿— | SLURM ä½œä¸šè¾“å‡º | æœ¬åœ°æ–‡ä»¶ |
| å¾ªç¯è¿è¡Œ | æ˜¯ï¼ˆæŒç»­ç›‘æ§ï¼‰ | å¦ï¼ˆè¿è¡Œä¸€æ¬¡ï¼‰ |

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å¹¶è¡ŒæœåŠ¡å™¨**: å¦‚æœæœ‰å¤šä¸ª GPUï¼Œå¯åŠ¨å¤šä¸ªæœåŠ¡å™¨å¯ä»¥åŠ å¿«è¯„æµ‹é€Ÿåº¦
   ```bash
   --num-servers 4  # 4 ä¸ª GPUï¼Œæ¯ä¸ªè¿è¡Œä¸€ä¸ªæœåŠ¡å™¨
   ```

2. **å‡å°‘å¯åŠ¨å»¶è¿Ÿ**: å¦‚æœ GPU å†…å­˜å……è¶³ï¼Œå¯ä»¥å‡å°‘å¯åŠ¨é—´éš”
   ```bash
   --stagger-delay 30  # ä»é»˜è®¤çš„ 60 ç§’å‡å°‘åˆ° 30 ç§’
   ```

3. **åˆ†æ‰¹è¯„æµ‹**: å¦‚æœæ—¶é—´æœ‰é™ï¼Œå¯ä»¥å…ˆè¯„æµ‹å•ä¸ªåŸŸ
   ```bash
   --domains retail  # åªè¯„æµ‹ retailï¼Œä¹‹åå†è¯„æµ‹å…¶ä»–åŸŸ
   ```

4. **è°ƒæ•´å¹¶å‘**: é€šè¿‡å¯åŠ¨å¤šä¸ªæœåŠ¡å™¨å®ä¾‹æ¥æé«˜ååé‡
   - 1 ä¸ª GPU: `--num-servers 1`
   - 4 ä¸ª GPU: `--num-servers 4`
   - 8 ä¸ª GPU: `--num-servers 8`

## ğŸ¯ å¿«é€Ÿå¼€å§‹æ£€æŸ¥æ¸…å•

- [ ] Conda ç¯å¢ƒ `vllm1` å·²åˆ›å»ºå¹¶æ¿€æ´»
- [ ] å·²å®‰è£…æ‰€éœ€ä¾èµ– (`pip install -e .`)
- [ ] ç¯å¢ƒå˜é‡å·²è®¾ç½® (`CKPT_DIR`, `REPO_PATH`, `OPENAI_API_KEY`)
- [ ] Agent æ¨¡å‹å·²ä¸‹è½½åˆ° `$CKPT_DIR`
- [ ] è‡³å°‘æœ‰ 1 ä¸ªå¯ç”¨ GPU
- [ ] è¿è¡Œæµ‹è¯•å‘½ä»¤éªŒè¯è®¾ç½®:
  ```bash
  python run_local.py --agent-model $CKPT_DIR --domains retail --num-trials 1
  ```

## ğŸ“ è·å–å¸®åŠ©

æŸ¥çœ‹æ‰€æœ‰å¯ç”¨é€‰é¡¹ï¼š
```bash
python run_local.py --help
```

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æœåŠ¡å™¨æ—¥å¿—: `logs/vllm_port_*.out` å’Œ `*.err`
2. ç¯å¢ƒå˜é‡: `echo $CKPT_DIR $REPO_PATH $OPENAI_API_KEY`
3. GPU çŠ¶æ€: `nvidia-smi`
