# Ï„Â²-Bench æœ¬åœ°è¿è¡ŒæŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•åœ¨æœ¬åœ°ç¯å¢ƒï¼ˆé SLURM é›†ç¾¤ï¼‰è¿è¡Œ Ï„Â²-Bench è¯„æµ‹ã€‚

---

## ğŸ“‹ ç›®å½•

1. [å‰ç½®è¦æ±‚](#å‰ç½®è¦æ±‚)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [å‘½ä»¤è¡Œå‚æ•°è¯¦è§£](#å‘½ä»¤è¡Œå‚æ•°è¯¦è§£)
4. [æ—¥å¿—ç³»ç»Ÿ](#æ—¥å¿—ç³»ç»Ÿ)
5. [è¾“å‡ºè¯´æ˜](#è¾“å‡ºè¯´æ˜)
6. [æ€§èƒ½åˆ†æ](#æ€§èƒ½åˆ†æ)
7. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## ğŸ“‹ å‰ç½®è¦æ±‚

### 1. ç¡¬ä»¶è¦æ±‚

- **GPU**: è‡³å°‘ 1 ä¸ª NVIDIA GPUï¼ˆæ¨è 4+ ä¸ªç”¨äºå¹¶è¡Œè¯„æµ‹ï¼‰
  - æ¯ä¸ª GPU è¿è¡Œä¸€ä¸ª vLLM server å®ä¾‹
  - GPU å†…å­˜è¦æ±‚å–å†³äºæ¨¡å‹å¤§å°ï¼ˆ8B æ¨¡å‹çº¦éœ€ 16-24GBï¼‰
- **CPU**: å¤šæ ¸ CPUï¼ˆæ¨è 16+ æ ¸å¿ƒï¼‰
- **å†…å­˜**: è‡³å°‘ 64GB RAM
- **å­˜å‚¨**: è‡³å°‘ 100GB å¯ç”¨ç©ºé—´ï¼ˆç”¨äºæ¨¡å‹ã€æ•°æ®å’Œæ—¥å¿—ï¼‰

### 2. Conda ç¯å¢ƒè®¾ç½®

#### æ–¹æ¡ˆ A: vLLM 0.9.2ï¼ˆç¨³å®šç‰ˆæœ¬ï¼‰

```bash
conda create -n vllm1 python=3.12 -y
conda activate vllm1
pip install torch
pip install "transformers<4.54.0"
pip install vllm==0.9.2
cd evaluation/tau2-bench
pip install -e .
```

#### æ–¹æ¡ˆ B: vLLM 0.10.1ï¼ˆBlackwell GPU æ”¯æŒï¼‰

```bash
conda create -n vllm1 python=3.12 -y
conda activate vllm1

# å¸è½½æ—§ç‰ˆæœ¬
pip uninstall -y vllm torch transformers

# å®‰è£… PyTorch 2.7.1 (CUDA 12.8)
pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 \
    --index-url https://download.pytorch.org/whl/cu128

# å®‰è£… vLLM 0.10.1 å’Œ transformers
pip install vllm==0.10.1 transformers

# vLLM 0.10.1+ éœ€è¦ hf_transfer åŠ é€Ÿä¸‹è½½
pip install hf_transfer

# å®‰è£… tau2-bench
cd evaluation/tau2-bench
pip install -e .
```

**æ³¨æ„**: transformers==4.57.3 + vLLM==0.10.1 å·²åœ¨ Blackwell GPU ä¸ŠéªŒè¯é€šè¿‡ã€‚

### 3. ç¯å¢ƒå˜é‡é…ç½®

é¡¹ç›®æ ¹ç›®å½•æä¾›äº† `setup_envs.sh` è„šæœ¬ï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€çš„ç¯å¢ƒå˜é‡ã€‚

#### 3.1 ç¼–è¾‘ setup_envs.sh

```bash
cd /path/to/ToolOrchestra
vim setup_envs.sh  # æˆ–ä½¿ç”¨å…¶ä»–ç¼–è¾‘å™¨
```

**å¿…éœ€çš„ç¯å¢ƒå˜é‡**ï¼š

```bash
# æ¨¡å‹å’Œè·¯å¾„
export CKPT_DIR="/path/to/your/agent/model"          # Agent æ¨¡å‹è·¯å¾„
export REPO_PATH="/path/to/ToolOrchestra"            # ä»“åº“æ ¹ç›®å½•
export HF_HOME="/path/to/huggingface_cache"          # HuggingFace ç¼“å­˜

# OpenAI APIï¼ˆå¿…éœ€ï¼Œç”¨äº user simulationï¼‰
export OPENAI_API_KEY="sk-..."                       # gpt-5 ç­‰æ¨¡å‹
```

**å¯é€‰çš„ç¯å¢ƒå˜é‡**ï¼š

```bash
# Anthropic APIï¼ˆå¦‚æœä½¿ç”¨ Claudeï¼‰
export ANTHROPIC_API_KEY="sk-ant-..."

# Together APIï¼ˆå¦‚æœä½¿ç”¨ Together AI æœåŠ¡ï¼‰
export TOGETHER_API_KEY="..."

# Nebius APIï¼ˆQwen3-32B å¿«é€Ÿæ¨ç†ï¼‰
# å¦‚æœè®¾ç½®ï¼ŒQwen3-32B ä¼šè‡ªåŠ¨ä½¿ç”¨ Nebius API è€Œä¸æ˜¯æœ¬åœ° vLLM
export NEBIUS_API_KEY="v1...."

# Tavily Search APIï¼ˆWeb æœç´¢åŠŸèƒ½ï¼‰
export TAVILY_KEY="tvly-..."

# WandBï¼ˆå®éªŒè·Ÿè¸ªï¼‰
export WANDB_API_KEY="..."
```

#### 3.2 åŠ è½½ç¯å¢ƒå˜é‡

```bash
cd /path/to/ToolOrchestra
source setup_envs.sh
```

è„šæœ¬ä¼šè‡ªåŠ¨éªŒè¯ç¯å¢ƒå¹¶æ˜¾ç¤ºé…ç½®ä¿¡æ¯ï¼š

```
=========================================
ToolOrchestra Environment Configuration
=========================================
REPO_PATH:    /workspace/ToolOrchestra
CKPT_DIR:     /workspace/ckpt/nvidia/Nemotron-Orchestrator-8B
HF_HOME:      /home/user/.cache/huggingface

API Keys:
  OPENAI_API_KEY:    sk-proj-EP...
  ANTHROPIC_API_KEY: sk-ant-...
  TOGETHER_API_KEY:  tgp_v1_IM...
  NEBIUS_API_KEY:    v1.CmQKHH... (Qwen3-32B)
  TAVILY_KEY:        tvly-dev-r...
=========================================
âœ“ Conda environment: vllm1
=========================================
```

### 4. æ¨¡å‹ä¸‹è½½

ä¸‹è½½ agent æ¨¡å‹åˆ° `$CKPT_DIR`ï¼š

```bash
# æ–¹æ³• 1: ä» HuggingFace ä¸‹è½½
huggingface-cli download nvidia/Nemotron-Orchestrator-8B \
    --local-dir $CKPT_DIR

# æ–¹æ³• 2: ä½¿ç”¨ git-lfs
git lfs clone https://huggingface.co/nvidia/Nemotron-Orchestrator-8B $CKPT_DIR

# éªŒè¯æ¨¡å‹æ–‡ä»¶
ls -lh $CKPT_DIR
# åº”è¯¥çœ‹åˆ°: config.json, model.safetensors, tokenizer.json ç­‰æ–‡ä»¶
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼

ç¡®ä¿å·²æ¿€æ´» conda ç¯å¢ƒå¹¶åŠ è½½ç¯å¢ƒå˜é‡åï¼š

```bash
# 1. æ¿€æ´»ç¯å¢ƒ
conda activate vllm1

# 2. åŠ è½½ç¯å¢ƒå˜é‡
cd /path/to/ToolOrchestra
source setup_envs.sh

# 3. è¿è¡Œè¯„æµ‹ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
cd evaluation/tau2-bench
python run_local.py --agent-model $CKPT_DIR
```

**é»˜è®¤è¡Œä¸º**ï¼š
- âœ… å¯åŠ¨ **4 ä¸ª vLLM servers**ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿ GPUï¼‰
- âœ… è¯„æµ‹ **retail, telecom, airline** ä¸‰ä¸ªåŸŸï¼ˆæ€»å…± 278 ä¸ªä»»åŠ¡ï¼‰
- âœ… ä½¿ç”¨ **gpt-5** ä½œä¸º user simulator
- âœ… ç»“æœä¿å­˜åœ¨ **outputs/** ç›®å½•
- âœ… æ˜¾ç¤ºå®æ—¶**è¿›åº¦æ¡**å’Œ ETA
- âœ… æ—¥å¿—çº§åˆ«ä¸º **INFO**
- âœ… **å¯ç”¨** expert routingï¼ˆ`--use_model_tool`ï¼Œä¸å®˜æ–¹ run.py å¯¹é½ï¼‰

**å¯ç”¨åŸŸ**: `mock` (9 ä»»åŠ¡), `retail` (114 ä»»åŠ¡), `telecom` (114 ä»»åŠ¡), `airline` (50 ä»»åŠ¡)

**ä¸å®˜æ–¹ run.py å®Œå…¨å¯¹é½**: 
- âœ… è¯„æµ‹çš„åŸŸå’Œä»»åŠ¡æ–‡ä»¶**å®Œå…¨ç›¸åŒ**
- âœ… é»˜è®¤å¯ç”¨ `--use_model_tool`ï¼ˆexpert routingï¼‰
- âœ… Agent å¯ä»¥è°ƒç”¨ expert æ¨¡å‹ï¼ˆgpt-5, gpt-5-mini, Qwen3-32Bï¼‰

**å¦‚æœä¸éœ€è¦ expert routing**ï¼ˆæ›´å¿«æ›´ä¾¿å®œï¼‰:
```bash
python run_local.py --agent-model $CKPT_DIR --no-use-model-tool
```

### ### æ¨èçš„å®Œæ•´å‘½ä»¤

**æ ‡å‡†è¯„æµ‹ï¼ˆä¸å®˜æ–¹ run.py å¯¹é½ï¼‰**:
```bash
python run_local.py \
  --agent-model $CKPT_DIR \
  --user-llm gpt-5 \
  --num-servers 4 \
  --domains retail telecom airline \
  --max-steps 200 \
  --num-trials 1 \
  --max-concurrency 10 \
  --log-level INFO \
  --output-dir outputs/run1 \
  --log-dir logs/run1
```

**è¯´æ˜**: 
- è¯„æµ‹ retail, telecom, airline ä¸‰ä¸ªä¸»è¦åŸŸï¼Œæ€»å…± 278 ä¸ªä»»åŠ¡ï¼ˆretail: 114, telecom: 114, airline: 50ï¼‰
- é»˜è®¤**å¯ç”¨** expert routingï¼ˆ`--use_model_tool`ï¼‰ï¼Œä¸å®˜æ–¹ run.py å®Œå…¨ä¸€è‡´
- Agent å¯ä»¥æ ¹æ®ä»»åŠ¡éš¾åº¦é€‰æ‹©è°ƒç”¨ expert æ¨¡å‹

**ä¸å¸¦ expert routing çš„ç®€åŒ–ç‰ˆæœ¬**ï¼ˆæ›´å¿«æ›´ä¾¿å®œï¼‰:
```bash
python run_local.py \
  --agent-model $CKPT_DIR \
  --user-llm gpt-5 \
  --num-servers 4 \
  --no-use-model-tool \
  --log-level INFO
```
è¿™ä¼šç¦ç”¨ expert routingï¼Œagent åªä½¿ç”¨è‡ªå·±çš„èƒ½åŠ›ã€‚

### Smoke Testï¼ˆå¿«é€ŸéªŒè¯ï¼‰

è¿è¡Œå°‘é‡ä»»åŠ¡éªŒè¯ç¯å¢ƒé…ç½®ï¼š

**æ–¹æ³• 1: ä½¿ç”¨ mock åŸŸï¼ˆæ¨èï¼‰**
```bash
# Mock åŸŸæœ‰å°‘é‡ç®€å•ä»»åŠ¡ï¼Œæœ€é€‚åˆå¿«é€ŸéªŒè¯
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains mock \
  --num-servers 1 \
  --log-level INFO
```

**æ–¹æ³• 2: é™åˆ¶ä»»åŠ¡æ•°é‡**
```bash
# åªè¯„æµ‹ retail åŸŸçš„å‰ 10 ä¸ªä»»åŠ¡
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains retail \
  --num-tasks 10 \
  --num-servers 1 \
  --log-level INFO
```

### Expert Routingï¼ˆé»˜è®¤å¯ç”¨ï¼‰

**é»˜è®¤æƒ…å†µä¸‹**ï¼Œexpert routing å·²å¯ç”¨ï¼ˆä¸å®˜æ–¹ run.py å¯¹é½ï¼‰ï¼š

```bash
# Expert routing é»˜è®¤å¯ç”¨ï¼ŒæŸ¥çœ‹è¯¦ç»†çš„ expert è°ƒç”¨
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains retail \
  --log-level PROFILE \
  2>&1 | tee eval_with_expert.log
```

**è¯´æ˜**: 
- `--use_model_tool` **é»˜è®¤å·²å¯ç”¨**ï¼ˆä¸å®˜æ–¹ run.py ä¸€è‡´ï¼‰
- Agent å¯ä»¥é€‰æ‹©è°ƒç”¨ä»¥ä¸‹ expert æ¨¡å‹:
  - **expert-1**: GPT-5ï¼ˆæœ€å¼ºï¼Œ$1.25/M inputï¼Œå¹³å‡ 96s å»¶è¿Ÿï¼‰
  - **expert-2**: GPT-5-miniï¼ˆä¸­ç­‰ï¼Œ$0.25/M inputï¼Œå¹³å‡ 27s å»¶è¿Ÿï¼‰
  - **expert-3**: Qwen3-32Bï¼ˆå¿«é€Ÿï¼Œ$0.8/M inputï¼Œå¹³å‡ 11s å»¶è¿Ÿï¼‰
- Agent ä¼šæ ¹æ®ä»»åŠ¡éš¾åº¦å’Œæˆæœ¬æƒè¡¡é€‰æ‹©åˆé€‚çš„ expert
- ä½¿ç”¨ PROFILE æ—¥å¿—å¯ä»¥çœ‹åˆ°æ¯æ¬¡ expert è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯ï¼š
  ```
  [PROFILE] ... type=expert_call model=gpt-5 call_type=openai duration_ms=2345.67
  ```

**ç¦ç”¨ expert routing**ï¼ˆæ›´å¿«æ›´ä¾¿å®œï¼Œä½†åç¦»å®˜æ–¹é…ç½®ï¼‰:
```bash
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains retail \
  --no-use-model-tool \
  --log-level INFO
```

è¿™ä¼šç¦ç”¨ call_expertï¼Œagent åªä½¿ç”¨è‡ªå·±çš„èƒ½åŠ›ï¼ŒèŠ‚çœ API æˆæœ¬ã€‚

---

## ğŸ“š å‘½ä»¤è¡Œå‚æ•°è¯¦è§£

### æ¨¡å‹é…ç½®å‚æ•°

#### `--agent-model PATH`
- **è¯´æ˜**: Agent æ¨¡å‹è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- **é»˜è®¤**: `$CKPT_DIR` ç¯å¢ƒå˜é‡
- **ç¤ºä¾‹**: `--agent-model /data/models/Nemotron-Orchestrator-8B`

#### `--user-llm MODEL`
- **è¯´æ˜**: User simulator ä½¿ç”¨çš„ LLM æ¨¡å‹
- **é»˜è®¤**: `gpt-5`
- **é€‰é¡¹**: 
  - `gpt-5`: GPT-4 çº§åˆ«ï¼ˆæ¨èï¼Œä¸ baseline ä¸€è‡´ï¼‰
  - `gpt-5-mini`: GPT-4 Miniï¼ˆæ›´å¿«æ›´ä¾¿å®œï¼‰
  - `claude-3-5-sonnet-20241022`: Claude Sonnet
  - `claude-4.1-opus`: Claude Opusï¼ˆæœ€å¼ºï¼‰
- **ç¤ºä¾‹**: `--user-llm gpt-5`
- **æ³¨æ„**: éœ€è¦è®¾ç½®å¯¹åº”çš„ API keyï¼ˆ`OPENAI_API_KEY` æˆ– `ANTHROPIC_API_KEY`ï¼‰

### Server é…ç½®å‚æ•°

#### `--num-servers N`
- **è¯´æ˜**: å¯åŠ¨çš„ vLLM server æ•°é‡
- **é»˜è®¤**: `4`
- **æ¨è**: ç­‰äºå¯ç”¨ GPU æ•°é‡
- **ç¤ºä¾‹**: 
  ```bash
  --num-servers 1   # 1 GPUï¼Œè°ƒè¯•æ¨¡å¼
  --num-servers 4   # 4 GPUï¼Œæ ‡å‡†é…ç½®
  --num-servers 8   # 8 GPUï¼Œé«˜ååé‡
  ```

#### `--start-port PORT`
- **è¯´æ˜**: Server èµ·å§‹ç«¯å£å·
- **é»˜è®¤**: `1900`
- **è¯´æ˜**: ç¬¬ i ä¸ª server ä½¿ç”¨ç«¯å£ `start-port + i`
- **ç¤ºä¾‹**: `--start-port 2000`  # ä½¿ç”¨ç«¯å£ 2000, 2001, 2002, ...

#### `--gpu-ids ID1 ID2 ...`
- **è¯´æ˜**: æŒ‡å®šä½¿ç”¨çš„ GPU ID
- **é»˜è®¤**: è‡ªåŠ¨ä½¿ç”¨ 0, 1, 2, ...
- **ç¤ºä¾‹**: `--gpu-ids 0 2 4 6`  # åªä½¿ç”¨å¶æ•° GPU

#### `--stagger-delay SECONDS`
- **è¯´æ˜**: Server å¯åŠ¨é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
- **é»˜è®¤**: `60`
- **è¯´æ˜**: é¿å… GPU OOMï¼Œé”™å¼€åŠ è½½æ¨¡å‹çš„æ—¶é—´
- **ç¤ºä¾‹**: `--stagger-delay 30`  # GPU å†…å­˜å……è¶³æ—¶å¯å‡å°‘

#### `--server-timeout SECONDS`
- **è¯´æ˜**: Server å¯åŠ¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- **é»˜è®¤**: `600` (10 åˆ†é’Ÿ)
- **ç¤ºä¾‹**: `--server-timeout 1200`  # å¤§æ¨¡å‹éœ€è¦æ›´é•¿æ—¶é—´

#### `--skip-server-start`
- **è¯´æ˜**: è·³è¿‡å¯åŠ¨ serverï¼Œä½¿ç”¨å·²è¿è¡Œçš„ server
- **é»˜è®¤**: ä¸è·³è¿‡
- **ä½¿ç”¨åœºæ™¯**: 
  - è°ƒè¯•æ—¶é¿å…é‡å¤å¯åŠ¨
  - æ‰‹åŠ¨ç®¡ç† server
- **è¦æ±‚**: éœ€è¦æ‰‹åŠ¨åˆ›å»º `model_config_local.json`

### è¯„æµ‹é…ç½®å‚æ•°

#### `--domains DOMAIN1 DOMAIN2 ...`
- **è¯´æ˜**: è¦è¯„æµ‹çš„åŸŸ
- **é»˜è®¤**: `retail telecom airline`ï¼ˆä¸»è¦çš„ä¸‰ä¸ªç”Ÿäº§åŸŸï¼Œ278 ä»»åŠ¡ï¼‰
- **å¯é€‰å€¼**: `mock`, `retail`, `telecom`, `airline`
- **åŸŸè¯´æ˜**:
  - `mock`: æµ‹è¯•åŸŸï¼ˆ9 ä¸ªç®€å•ä»»åŠ¡ï¼Œç”¨äºéªŒè¯ç¯å¢ƒï¼‰
  - `retail`: é›¶å”®å®¢æœï¼ˆ114 ä»»åŠ¡ï¼‰
  - `telecom`: ç”µä¿¡å®¢æœï¼ˆ114 ä»»åŠ¡ï¼‰
  - `airline`: èˆªç©ºå®¢æœï¼ˆ50 ä»»åŠ¡ï¼‰
- **ç¤ºä¾‹**: 
  ```bash
  --domains mock               # åªè¯„æµ‹ mockï¼ˆ9 ä»»åŠ¡ï¼Œå¿«é€Ÿæµ‹è¯•ï¼‰
  --domains retail             # åªè¯„æµ‹ retailï¼ˆ114 ä»»åŠ¡ï¼‰
  --domains retail telecom     # è¯„æµ‹ä¸¤ä¸ªåŸŸï¼ˆ228 ä»»åŠ¡ï¼‰
  --domains mock retail telecom airline  # è¯„æµ‹æ‰€æœ‰å¯ç”¨åŸŸï¼ˆ287 ä»»åŠ¡ï¼‰
  ```
- **æ³¨æ„**: å…¶ä»–åŸŸï¼ˆbank, medicine, movie ç­‰ï¼‰è™½ç„¶åœ¨ registry ä¸­æ³¨å†Œï¼Œä½†**æš‚æ— ä»»åŠ¡æ–‡ä»¶**ï¼Œæ— æ³•é€šè¿‡ run_local.py è¯„æµ‹

#### `--max-steps N`
- **è¯´æ˜**: æ¯ä¸ªä»»åŠ¡çš„æœ€å¤§æ­¥æ•°
- **é»˜è®¤**: `200`
- **è¯´æ˜**: è¾¾åˆ°æœ€å¤§æ­¥æ•°ä¼šç»ˆæ­¢ä»»åŠ¡ï¼ˆtermination_reason: MAX_STEPSï¼‰
- **ç¤ºä¾‹**: `--max-steps 100`  # å¿«é€Ÿæµ‹è¯•

#### `--num-trials N`
- **è¯´æ˜**: æ¯ä¸ªä»»åŠ¡è¿è¡Œçš„æ¬¡æ•°
- **é»˜è®¤**: `1`
- **è¯´æ˜**: ç”¨äºè¯„ä¼° agent ç¨³å®šæ€§ï¼ˆå¤šæ¬¡è¿è¡Œå–å¹³å‡ï¼‰
- **ç¤ºä¾‹**: `--num-trials 3`  # æ¯ä¸ªä»»åŠ¡è¿è¡Œ 3 æ¬¡

#### `--num-tasks N`
- **è¯´æ˜**: åªè¿è¡Œå‰ N ä¸ªä»»åŠ¡ï¼ˆsmoke testï¼‰
- **é»˜è®¤**: `None`ï¼ˆè¿è¡Œæ‰€æœ‰ä»»åŠ¡ï¼‰
- **ç¤ºä¾‹**: 
  ```bash
  --num-tasks 10   # åªè¿è¡Œå‰ 10 ä¸ªä»»åŠ¡
  --num-tasks 50   # å¿«é€ŸéªŒè¯
  ```

#### `--max-concurrency N`
- **è¯´æ˜**: æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
- **é»˜è®¤**: `10`
- **è¯´æ˜**: 
  - å¤ªé«˜: å¯èƒ½å¯¼è‡´ GPU OOM æˆ– server è´Ÿè½½è¿‡é«˜
  - å¤ªä½: é™ä½è¯„æµ‹é€Ÿåº¦
- **æ¨è**: `num-servers * 2` åˆ° `num-servers * 3`
- **ç¤ºä¾‹**: 
  ```bash
  --max-concurrency 1    # è°ƒè¯•æ¨¡å¼ï¼ˆä¸²è¡Œæ‰§è¡Œï¼‰
  --max-concurrency 10   # æ ‡å‡†é…ç½®
  --max-concurrency 20   # 8 ä¸ª server æ—¶çš„é«˜å¹¶å‘
  ```

#### `--use_model_tool` / `--no-use-model-tool`
- **è¯´æ˜**: å¯ç”¨/ç¦ç”¨ expert routingï¼ˆcall_expert åŠŸèƒ½ï¼‰
- **é»˜è®¤**: **å¯ç”¨**ï¼ˆä¸å®˜æ–¹ run.py å¯¹é½ï¼‰
- **è¯´æ˜**: Agent å¯ä»¥è°ƒç”¨æ›´å¼ºçš„ expert æ¨¡å‹ï¼ˆgpt-5, gpt-5-mini, Qwen3-32Bï¼‰
- **ç¤ºä¾‹**: 
  ```bash
  --use_model_tool           # å¯ç”¨ï¼ˆé»˜è®¤ï¼‰
  --no-use-model-tool        # ç¦ç”¨ï¼ˆæ›´å¿«æ›´ä¾¿å®œï¼‰
  ```
- **Expert æ¨¡å‹é€‰é¡¹**:
  - expert-1: GPT-5ï¼ˆ$1.25/M input, ~96s å»¶è¿Ÿï¼‰
  - expert-2: GPT-5-miniï¼ˆ$0.25/M input, ~27s å»¶è¿Ÿï¼‰
  - expert-3: Qwen3-32Bï¼ˆ$0.8/M input, ~11s å»¶è¿Ÿï¼‰
- **æ³¨æ„**: å¯ç”¨ä¼šå¢åŠ  API è°ƒç”¨æˆæœ¬å’Œå»¶è¿Ÿï¼Œä½†å¯èƒ½æé«˜ä»»åŠ¡æˆåŠŸç‡

#### `--max-errors N`
- **è¯´æ˜**: å…è®¸çš„è¿ç»­ tool error æ¬¡æ•°
- **é»˜è®¤**: `10`
- **è¯´æ˜**: è¾¾åˆ°ä¸Šé™ä¼šç»ˆæ­¢ä»»åŠ¡ï¼ˆtermination_reason: TOO_MANY_ERRORSï¼‰
- **ç¤ºä¾‹**: `--max-errors 5`

#### `--seed N`
- **è¯´æ˜**: éšæœºç§å­ï¼ˆreproducibilityï¼‰
- **é»˜è®¤**: `300`
- **è¯´æ˜**: ä¸ tau2-bench baseline ä¸€è‡´
- **ç¤ºä¾‹**: `--seed 42`

### æ—¥å¿—é…ç½®å‚æ•°

#### `--log-level LEVEL`
- **è¯´æ˜**: æ—¥å¿—çº§åˆ«
- **é»˜è®¤**: `INFO`
- **å¯é€‰å€¼**: `DEBUG`, `PROFILE`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`
- **è¯¦ç»†è¯´æ˜**:
  - **`DEBUG`**: æ˜¾ç¤ºæ‰€æœ‰ç»†èŠ‚ï¼ˆagent/ç¯å¢ƒåˆå§‹åŒ–ã€æ¯ä¸ª stepï¼‰
  - **`PROFILE`**: è®°å½•æ€§èƒ½æ•°æ®ï¼ˆLLM calls, tool calls, step timingï¼‰ç²¾ç¡®åˆ°æ¯«ç§’
  - **`INFO`**: æ˜¾ç¤ºè¿›åº¦æ¡å’ŒåŸºæœ¬ä¿¡æ¯ï¼ˆæ¨èç”¨äºæ­£å¸¸è¯„æµ‹ï¼‰
  - **`WARNING`**: åªæ˜¾ç¤ºè­¦å‘Šå’Œæ›´ä¸¥é‡çš„ä¿¡æ¯
  - **`ERROR`**: åªæ˜¾ç¤ºé”™è¯¯
- **ç¤ºä¾‹**: 
  ```bash
  --log-level INFO       # æ­£å¸¸è¯„æµ‹
  --log-level PROFILE    # æ€§èƒ½åˆ†æ
  --log-level DEBUG      # è°ƒè¯•é—®é¢˜
  ```

#### `--log-dir DIR`
- **è¯´æ˜**: vLLM server æ—¥å¿—ç›®å½•
- **é»˜è®¤**: `logs`
- **ç¤ºä¾‹**: `--log-dir logs/run1`

### è¾“å‡ºé…ç½®å‚æ•°

#### `--output-dir DIR`
- **è¯´æ˜**: è¯„æµ‹ç»“æœä¿å­˜ç›®å½•
- **é»˜è®¤**: `outputs`
- **è¯´æ˜**: æ¯ä¸ªåŸŸçš„ç»“æœä¿å­˜ä¸º `{domain}.json`
- **ç¤ºä¾‹**: `--output-dir outputs/experiment1`

#### `--model-config-path FILE`
- **è¯´æ˜**: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
- **é»˜è®¤**: `model_config_local.json`
- **è¯´æ˜**: è‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…å«æ‰€æœ‰ server çš„ IP å’Œç«¯å£ä¿¡æ¯
- **ç¤ºä¾‹**: `--model-config-path configs/my_config.json`

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### ä½¿ç”¨å·²è¿è¡Œçš„ vLLM Server

å¦‚æœå·²ç»æ‰‹åŠ¨å¯åŠ¨äº† vLLM serverï¼Œå¯ä»¥è·³è¿‡å¯åŠ¨æ­¥éª¤ï¼š

#### æ­¥éª¤ 1: æ‰‹åŠ¨å¯åŠ¨ vLLM Servers

```bash
# Terminal 1: å¯åŠ¨ç¬¬ä¸€ä¸ª server
CUDA_VISIBLE_DEVICES=0 vllm serve $CKPT_DIR \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --port 1900

# Terminal 2: å¯åŠ¨ç¬¬äºŒä¸ª server
CUDA_VISIBLE_DEVICES=1 vllm serve $CKPT_DIR \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --port 1901
```

#### æ­¥éª¤ 2: åˆ›å»ºé…ç½®æ–‡ä»¶

```bash
cat > model_config_local.json << EOF
{
  "$CKPT_DIR": [
    {"ip_addr": "127.0.0.1", "port": "1900"},
    {"ip_addr": "127.0.0.1", "port": "1901"}
  ],
  "vllm_model_config_path": "model_config_local.json"
}
EOF
```

#### æ­¥éª¤ 3: è¿è¡Œè¯„æµ‹

```bash
python run_local.py \
  --agent-model $CKPT_DIR \
  --skip-server-start \
  --model-config-path model_config_local.json
```

### å¤šæ¬¡è¿è¡ŒåŒä¸€åŸŸ

è¯„ä¼° agent çš„ç¨³å®šæ€§ï¼ˆvarianceï¼‰ï¼š

```bash
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains retail \
  --num-trials 5 \
  --seed 42
```

æ¯ä¸ªä»»åŠ¡ä¼šè¿è¡Œ 5 æ¬¡ï¼Œä½¿ç”¨ä¸åŒçš„éšæœºç§å­ã€‚

### åˆ†åŸŸè¿è¡Œå¹¶åˆå¹¶ç»“æœ

å¤§è§„æ¨¡è¯„æµ‹æ—¶ï¼Œå¯ä»¥åˆ†åŸŸè¿è¡Œä»¥ä¾¿ä¸­æ–­æ¢å¤ï¼š

```bash
# è¿è¡Œ retail
python run_local.py --agent-model $CKPT_DIR --domains retail \
  --output-dir outputs/batch1

# è¿è¡Œ telecom
python run_local.py --agent-model $CKPT_DIR --domains telecom \
  --output-dir outputs/batch1

# è¿è¡Œ airline
python run_local.py --agent-model $CKPT_DIR --domains airline \
  --output-dir outputs/batch1

# ç»“æœä¼šä¿å­˜åœ¨ outputs/batch1/ ä¸‹
```

### å®Œæ•´ç¤ºä¾‹å‘½ä»¤

#### åŸºç¡€è¯„æµ‹ï¼ˆæ­£å¸¸è¿è¡Œï¼‰

```bash
python run_local.py \
  --agent-model $CKPT_DIR \
  --user-llm gpt-5 \
  --num-servers 4 \
  --domains retail telecom airline \
  --max-steps 200 \
  --num-trials 1 \
  --max-concurrency 10 \
  --log-level INFO \
  --output-dir outputs/baseline_run \
  --log-dir logs/baseline_run
```

#### æ€§èƒ½åˆ†æè¯„æµ‹ï¼ˆPROFILE æ—¥å¿—ï¼‰

```bash
python run_local.py \
  --agent-model $CKPT_DIR \
  --user-llm gpt-5 \
  --num-servers 4 \
  --domains retail \
  --max-steps 200 \
  --num-trials 1 \
  --max-concurrency 10 \
  --log-level PROFILE \
  --output-dir outputs/profile_run \
  --log-dir logs/profile_run \
  2>&1 | tee evaluation_profile.log

# æå– PROFILE æ•°æ®ç”¨äºåˆ†æ
grep "\[PROFILE\]" evaluation_profile.log > profile_data.log
grep "\[USER_JUDGE\]" evaluation_profile.log > judge_data.log
```

#### æŸ¥çœ‹ Expert Routing è¯¦æƒ…

```bash
# Expert routing é»˜è®¤å¯ç”¨ï¼Œä½¿ç”¨ PROFILE æŸ¥çœ‹è¯¦ç»†è°ƒç”¨ä¿¡æ¯
python run_local.py \
  --agent-model $CKPT_DIR \
  --user-llm gpt-5 \
  --num-servers 4 \
  --domains retail \
  --log-level PROFILE \
  --output-dir outputs/with_expert \
  2>&1 | tee expert_eval.log

# åˆ†æ expert è°ƒç”¨æƒ…å†µ
grep "type=expert_call" expert_eval.log | wc -l
grep "type=expert_call" expert_eval.log | head -10
```

#### å¿«é€Ÿ Smoke Test

```bash
# æ–¹æ³• 1: ä½¿ç”¨ mock åŸŸï¼ˆ9 ä¸ªä»»åŠ¡ï¼Œç¦ç”¨ expert åŠ å¿«é€Ÿåº¦ï¼‰
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains mock \
  --num-servers 1 \
  --max-concurrency 1 \
  --no-use-model-tool \
  --log-level DEBUG

# æ–¹æ³• 2: retail å‰ 10 ä¸ªä»»åŠ¡ï¼ˆå¸¦ expertï¼‰
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains retail \
  --num-tasks 10 \
  --num-servers 1 \
  --max-concurrency 1 \
  --log-level DEBUG
```

#### é«˜å¹¶å‘å¤§è§„æ¨¡è¯„æµ‹

```bash
# 8 GPUï¼Œé«˜å¹¶å‘ï¼Œè¯„æµ‹æ‰€æœ‰åŸŸï¼ˆå¸¦ expertï¼Œé»˜è®¤ï¼‰
python run_local.py \
  --agent-model $CKPT_DIR \
  --user-llm gpt-5 \
  --num-servers 8 \
  --domains retail telecom airline \
  --max-concurrency 24 \
  --stagger-delay 30 \
  --log-level INFO \
  --output-dir outputs/full_eval \
  --log-dir logs/full_eval

# ä¸å¸¦ expertï¼ˆæ›´å¿«ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£ï¼‰
python run_local.py \
  --agent-model $CKPT_DIR \
  --user-llm gpt-5 \
  --num-servers 8 \
  --domains retail telecom airline \
  --max-concurrency 24 \
  --no-use-model-tool \
  --log-level INFO \
  --output-dir outputs/fast_eval \
  --log-dir logs/fast_eval
```

---

## ğŸ“Š æ—¥å¿—ç³»ç»Ÿ

> **é«˜çº§ç”¨æˆ·**: æœ‰å…³æ—¥å¿—æ¶æ„ã€Profile æ•°æ®æ ¼å¼å’Œè°ƒè¯•å·¥ä½œæµçš„è¯¦ç»†æŠ€æœ¯è¯´æ˜ï¼Œè¯·å‚é˜…ä¸“é—¨çš„æ–‡æ¡£ [LOGGING_AND_DEBUGGING.md](./LOGGING_AND_DEBUGGING.md)ã€‚

### æ—¥å¿—çº§åˆ«å±‚æ¬¡

```
DEBUG (10)      â† æœ€è¯¦ç»†
PROFILE (15)    â† æ€§èƒ½åˆ†æ
USER_JUDGE (15) â† è¯„æµ‹æ¡†æ¶
INFO (20)       â† é»˜è®¤
WARNING (30)
ERROR (40)
CRITICAL (50)   â† æœ€ä¸¥é‡
```

### å„çº§åˆ«è¯´æ˜

#### DEBUG Level
**ç”¨é€”**: è°ƒè¯• agent è¡Œä¸ºå’Œç¯å¢ƒé—®é¢˜

**è¾“å‡ºå†…å®¹**:
- Agent/Environment åˆå§‹åŒ–è¯¦æƒ…
- æ¯ä¸ª step çš„è¯¦ç»†ä¿¡æ¯ï¼ˆfrom_role â†’ to_roleï¼‰
- Message ä¼ é€’è¿‡ç¨‹
- å·¥å…·åŒæ­¥ï¼ˆsync_toolsï¼‰

**ä½¿ç”¨åœºæ™¯**: 
- Agent è¡Œä¸ºå¼‚å¸¸
- Task å¤±è´¥éœ€è¦å®šä½åŸå› 
- å¼€å‘æ–°åŠŸèƒ½

#### PROFILE Level
**ç”¨é€”**: æ€§èƒ½åˆ†æå’Œä¼˜åŒ–

**è¾“å‡ºå†…å®¹**:
- **LLM calls**: Agent ä¸»æ¨¡å‹è°ƒç”¨
  - `type=llm_call`
  - `model`, `call_type` (vllm/openai/claude)
  - `duration_ms`: ç²¾ç¡®åˆ°æ¯«ç§’
  - `has_tool_calls`: æ˜¯å¦æœ‰ tool è°ƒç”¨
- **Expert calls**: Expert æ¨¡å‹è°ƒç”¨ï¼ˆå¦‚æœå¯ç”¨ `--use_model_tool`ï¼‰
  - `type=expert_call`
  - `model` (gpt-5/gpt-5-mini/Qwen3-32B)
  - `duration_ms`
- **Tool calls**: æœ¬åœ° Python å‡½æ•°æ‰§è¡Œ
  - `type=tool_call`
  - `function`: å‡½æ•°å
  - `call_type=local_function`
  - `duration_ms`
  - `error`: æ˜¯å¦æœ‰é”™è¯¯
- **Step completion**: å®Œæ•´ step æ‰§è¡Œæ—¶é—´
  - `type=step_complete`
  - `total_duration_ms`: LLM + tool æ€»æ—¶é—´
  - `from_role`, `to_role`

**ä½¿ç”¨åœºæ™¯**:
- åˆ†ææ€§èƒ½ç“¶é¢ˆ
- æ¯”è¾ƒä¸åŒæ¨¡å‹çš„å»¶è¿Ÿ
- ä¼˜åŒ– tool call æ•ˆç‡
- è®¡ç®— cost å’Œ latency ç»Ÿè®¡

#### USER_JUDGE Level
**ç”¨é€”**: è®°å½•è¯„æµ‹æ¡†æ¶çš„ LLM è°ƒç”¨

**è¾“å‡ºå†…å®¹**:
- **User simulator**: `type=user_sim`
- **LLM-as-judge**: `type=evaluator`
- åŒ…å« `model`, `call_type`, `duration_ms`

**ä½¿ç”¨åœºæ™¯**:
- åˆ†æè¯„æµ‹æˆæœ¬
- User simulator æ€§èƒ½åˆ†æ
- Judge model è°ƒç”¨ç»Ÿè®¡

#### INFO Levelï¼ˆé»˜è®¤ï¼‰
**ç”¨é€”**: æ­£å¸¸è¯„æµ‹

**è¾“å‡ºå†…å®¹**:
- å®æ—¶è¿›åº¦æ¡
- ä»»åŠ¡å®Œæˆä¿¡æ¯
- è¯„æµ‹å¼€å§‹/ç»“æŸ

**ä½¿ç”¨åœºæ™¯**: å¤§éƒ¨åˆ†æ­£å¸¸è¯„æµ‹

## ğŸ“Š è¾“å‡ºè¯´æ˜

### 1. å®æ—¶è¿›åº¦æ¡

è¿è¡Œæ—¶ä¼šæ˜¾ç¤ºå®æ—¶è¿›åº¦æ¡ï¼ˆä½¿ç”¨ `rich` åº“ï¼‰ï¼š
```
  â ‹ Eval retail (114 tasks) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42/114 37% [00:15:23 / 00:25:42]
```

**è¿›åº¦æ¡å…ƒç´ **:
- **Spinner** (â ‹): è¡¨ç¤ºæ­£åœ¨è¿è¡Œ
- **Domain**: å½“å‰è¯„æµ‹çš„åŸŸ
- **Task count**: (å·²å®Œæˆ/æ€»ä»»åŠ¡æ•°)
- **Progress bar**: å¯è§†åŒ–è¿›åº¦
- **Percentage**: å®Œæˆç™¾åˆ†æ¯”
- **Time elapsed**: å·²ç”¨æ—¶é—´ï¼ˆå®é™…å·²èŠ±è´¹ï¼‰
- **Time remaining**: é¢„è®¡å‰©ä½™æ—¶é—´ï¼ˆETAï¼ŒåŸºäºå½“å‰é€Ÿåº¦ä¼°ç®—ï¼‰

**æ›´æ–°é¢‘ç‡**: æ¯ 10 ç§’è‡ªåŠ¨æ›´æ–°ï¼Œæˆ–ä»»åŠ¡å®Œæˆæ—¶ç«‹å³æ›´æ–°

### 2. Console æ—¥å¿—è¾“å‡º

#### INFO Level è¾“å‡ºç¤ºä¾‹

```
[2025-12-23 10:15:00] Starting 4 vLLM server(s)...
[2025-12-23 10:15:01] Starting vLLM server: /workspace/ckpt/Orchestrator-8B
[2025-12-23 10:15:01]   GPU: 0, Port: 1900
[2025-12-23 10:16:30] âœ“ Server on port 1900 is ready (took 89s)
[2025-12-23 10:16:30] Model configuration written to model_config_local.json
[INFO] 2025-12-23 10:16:31.123 task=global thread=140235 Starting evaluation: 278 tasks with max_concurrency=10
  â ‹ Eval retail (114 tasks) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 114/114 100% [01:23:45 / 00:00:00]
  â ‹ Eval telecom (114 tasks) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 114/114 100% [01:28:12 / 00:00:00]
  â ‹ Eval airline (50 tasks) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 50/50 100% [00:35:30 / 00:00:00]
[INFO] 2025-12-23 13:44:28 task=global thread=140235 Completed 278 simulations
[2025-12-23 13:44:30] ========== EVALUATION SUMMARY ==========
[2025-12-23 13:44:30] RETAIL: SUCCESS
[2025-12-23 13:44:30] TELECOM: SUCCESS
[2025-12-23 13:44:30] AIRLINE: SUCCESS
[2025-12-23 13:44:30] ========================================
```

#### PROFILE Level è¾“å‡ºç¤ºä¾‹

ä½¿ç”¨ `--log-level PROFILE` æ—¶ï¼Œä¼šçœ‹åˆ°è¯¦ç»†çš„æ€§èƒ½è®¡æ—¶ä¿¡æ¯ï¼š

```
[PROFILE] 2025-12-23 10:15:32.456 task=retail_001 thread=12345 type=llm_call model=nemotron call_type=vllm step=5 duration_ms=1234.56 has_tool_calls=True
[PROFILE] 2025-12-23 10:15:32.789 task=retail_001 thread=12345 type=expert_call model=gpt-5 call_type=openai duration_ms=2345.67
[PROFILE] 2025-12-23 10:15:33.012 task=retail_001 thread=12345 type=tool_call function=get_order_details call_type=local_function step=5 duration_ms=12.34
[PROFILE] 2025-12-23 10:15:33.015 task=retail_001 thread=12345 type=step_complete step=5 total_duration_ms=3591.00 from_role=agent to_role=env
[USER_JUDGE] 2025-12-23 10:15:34.100 task=retail_001 thread=12345 type=user_sim model=gpt-5 call_type=openai duration_ms=890.12
[USER_JUDGE] 2025-12-23 10:15:40.500 task=retail_001 thread=12345 type=evaluator model=gpt-5 call_type=openai duration_ms=1200.34
```

**æ—¥å¿—å­—æ®µè¯´æ˜**:
- `task`: å½“å‰ä»»åŠ¡ ID
- `thread`: çº¿ç¨‹ IDï¼ˆæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œï¼‰
- `type`: äº‹ä»¶ç±»å‹
  - `llm_call`: Agent ä¸»æ¨¡å‹è°ƒç”¨
  - `expert_call`: Expert æ¨¡å‹è°ƒç”¨ï¼ˆé€šè¿‡ call_expertï¼‰
  - `tool_call`: æœ¬åœ° Python å‡½æ•°è°ƒç”¨
  - `step_complete`: å®Œæ•´ step æ‰§è¡Œå®Œæˆ
  - `user_sim`: User simulator è°ƒç”¨
  - `evaluator`: LLM-as-judge è¯„ä¼°å™¨è°ƒç”¨
- `model`: ä½¿ç”¨çš„æ¨¡å‹åç§°
- `call_type`: è°ƒç”¨ç±»å‹ï¼ˆ`vllm`/`openai`/`claude`/`local_function`ï¼‰
- `duration_ms`: Wall clock è¿è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
- `step`: å½“å‰ step ç¼–å·

#### ä½¿ç”¨ PROFILE æ—¥å¿—è¿›è¡Œæ€§èƒ½åˆ†æ

PROFILE æ—¥å¿—å¯ä»¥ç”¨äºï¼š
1. **åˆ†æ tool call æ—¶é—´åˆ†å¸ƒ**: ç­›é€‰ `type=tool_call` çš„è¡Œ
2. **åˆ†æä¸åŒ LLM è°ƒç”¨çš„å»¶è¿Ÿ**: ç­›é€‰ `type=llm_call` å’Œ `type=expert_call`
3. **è®¡ç®—ç»Ÿè®¡ä¿¡æ¯**: ä½¿ç”¨è„šæœ¬æå– `duration_ms` è®¡ç®— mean/variance

ç¤ºä¾‹åˆ†æè„šæœ¬ï¼š
```bash
# æå–æ‰€æœ‰ PROFILE æ—¥å¿—
grep "\[PROFILE\]" output.log > profile.log

# ç»Ÿè®¡å„ç±» tool call çš„å¹³å‡æ—¶é—´
grep "type=tool_call" profile.log | \
  awk '{for(i=1;i<=NF;i++){if($i~/duration_ms=/){print $i}}}' | \
  cut -d'=' -f2 | \
  awk '{sum+=$1; count++} END {print "Mean:", sum/count, "ms"}'

# ç»Ÿè®¡ LLM call çš„æ€»æ—¶é—´
grep "type=llm_call" profile.log | \
  awk '{for(i=1;i<=NF;i++){if($i~/duration_ms=/){print $i}}}' | \
  cut -d'=' -f2 | \
  awk '{sum+=$1} END {print "Total:", sum, "ms"}'
```

### æ–‡ä»¶è¾“å‡ºç»“æ„

```
evaluation/tau2-bench/
â”œâ”€â”€ outputs/                          # è¯„æµ‹ç»“æœ
â”‚   â””â”€â”€ retail.json                   # Retail åŸŸå®Œæ•´ç»“æœ
â”œâ”€â”€ logs/                              # vLLM server æ—¥å¿—
â”‚   â”œâ”€â”€ vllm_port_1900_*.out          # Server stdout
â”‚   â””â”€â”€ vllm_port_1900_*.err          # Server stderr
â”œâ”€â”€ model_config_local.json            # æ¨¡å‹é…ç½®ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â””â”€â”€ evaluation.log                     # è¯„æµ‹æ—¥å¿—ï¼ˆå¦‚æœä½¿ç”¨ teeï¼‰
```

#### outputs/{domain}.json ç»“æ„

```json
{
  "info": {
    "agent_info": {"implementation": "llm_agent", "llm": "..."},
    "user_info": {"implementation": "user_simulator", "llm": "gpt-5"},
    "max_steps": 200,
    "num_trials": 1
  },
  "tasks": [...],  // Task definitions
  "simulations": [  // Simulation results
    {
      "id": "uuid",
      "task_id": "retail_001",
      "duration": 75.23,
      "termination_reason": "agent_stop",
      "reward_info": {"reward": 1.0},
      "messages": [...]
    }
  ]
}
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### é—®é¢˜ 1: GPU å†…å­˜ä¸è¶³

**é”™è¯¯ä¿¡æ¯**:
```
CUDA out of memory
torch.cuda.OutOfMemoryError
```

**åŸå› **: GPU å†…å­˜ä¸è¶³ä»¥åŠ è½½æ¨¡å‹æˆ–å¤„ç†å¹¶å‘è¯·æ±‚

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°‘ server æ•°é‡:
   ```bash
   --num-servers 1
   ```

2. é™ä½å¹¶å‘æ•°:
   ```bash
   --max-concurrency 1
   ```

3. æ£€æŸ¥ GPU å†…å­˜ä½¿ç”¨:
   ```bash
   nvidia-smi
   ```

4. æ¸…ç† GPU å†…å­˜:
   ```bash
   # åœæ­¢æ‰€æœ‰ vLLM è¿›ç¨‹
   pkill -9 -f vllm
   ```

#### é—®é¢˜ 2: vLLM Server å¯åŠ¨è¶…æ—¶

**é”™è¯¯ä¿¡æ¯**:
```
âœ— Server on port 1900 failed to start within 600s
```

**åŸå› **: æ¨¡å‹åŠ è½½æ—¶é—´è¿‡é•¿æˆ– server å¯åŠ¨å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ è¶…æ—¶æ—¶é—´:
   ```bash
   --server-timeout 1200  # 20 åˆ†é’Ÿ
   ```

2. æ£€æŸ¥ server æ—¥å¿—:
   ```bash
   tail -100 logs/vllm_port_1900_*.out
   cat logs/vllm_port_1900_*.err
   ```

3. éªŒè¯æ¨¡å‹è·¯å¾„:
   ```bash
   ls -lh $CKPT_DIR
   # åº”è¯¥çœ‹åˆ°: config.json, model.safetensors, tokenizer.json
   ```

4. æ‰‹åŠ¨æµ‹è¯• server å¯åŠ¨:
   ```bash
   CUDA_VISIBLE_DEVICES=0 vllm serve $CKPT_DIR \
     --enable-auto-tool-choice \
     --tool-call-parser hermes \
     --port 1900
   ```

#### é—®é¢˜ 3: ç¯å¢ƒå˜é‡æœªè®¾ç½®

**é”™è¯¯ä¿¡æ¯**:
```
ERROR: Agent model path not specified
ERROR: Task file not found
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $CKPT_DIR
echo $REPO_PATH
echo $OPENAI_API_KEY

# åŠ è½½ç¯å¢ƒå˜é‡
cd /path/to/ToolOrchestra
source setup_envs.sh
```

#### é—®é¢˜ 4: API Key é”™è¯¯

**é”™è¯¯ä¿¡æ¯**:
```
API key not found
Authentication failed
Incorrect API key provided
```

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥ API key:
   ```bash
   echo $OPENAI_API_KEY | cut -c1-10
   ```

2. éªŒè¯ API key:
   ```bash
   # æµ‹è¯• OpenAI API
   curl https://api.openai.com/v1/models \
     -H "Authorization: Bearer $OPENAI_API_KEY"
   ```

3. æ›´æ–° API key:
   ```bash
   export OPENAI_API_KEY="sk-proj-..."
   ```

#### é—®é¢˜ 5: ç«¯å£å·²è¢«å ç”¨

**é”™è¯¯ä¿¡æ¯**:
```
Address already in use
OSError: [Errno 48] Address already in use
```

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥ç«¯å£å ç”¨:
   ```bash
   lsof -i:1900
   netstat -tuln | grep 1900
   ```

2. åœæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹:
   ```bash
   lsof -ti:1900 | xargs kill -9
   ```

3. ä½¿ç”¨ä¸åŒç«¯å£:
   ```bash
   --start-port 2000
   ```

#### é—®é¢˜ 6: Task å¤±è´¥æˆ– Reward = 0

**ç°è±¡**: æ‰€æœ‰æˆ–å¤§éƒ¨åˆ† task çš„ reward éƒ½æ˜¯ 0

**å¯èƒ½åŸå› **:
- Agent æ²¡æœ‰æ­£ç¡®è°ƒç”¨ tools
- User simulator è´¨é‡é—®é¢˜
- è¯„æµ‹æ ‡å‡†ä¸åŒ¹é…

**è°ƒè¯•æ­¥éª¤**:
1. ä½¿ç”¨ DEBUG çº§åˆ«æŸ¥çœ‹è¯¦ç»†æ‰§è¡Œè¿‡ç¨‹:
   ```bash
   --log-level DEBUG --num-tasks 1
   ```

2. æ£€æŸ¥ä¸€ä¸ªå¤±è´¥ task çš„ messages:
   ```python
   import json
   with open('outputs/retail.json') as f:
       data = json.load(f)
   # æŸ¥çœ‹ç¬¬ä¸€ä¸ª simulation çš„ messages
   print(data['simulations'][0]['messages'])
   ```

3. éªŒè¯ agent æ˜¯å¦æ­£ç¡®åŠ è½½å·¥å…·å®šä¹‰

#### é—®é¢˜ 7: è¿›åº¦æ¡å¡ä½ä¸åŠ¨

**ç°è±¡**: è¿›åº¦æ¡é•¿æ—¶é—´ä¸æ›´æ–°

**å¯èƒ½åŸå› **:
- Task æ‰§è¡Œæ—¶é—´è¿‡é•¿
- LLM API å“åº”æ…¢
- å¹¶å‘æ•°è®¾ç½®è¿‡é«˜å¯¼è‡´æ’é˜Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„è¿›ç¨‹:
   ```bash
   ps aux | grep python
   ps aux | grep vllm
   ```

2. æŸ¥çœ‹æœ€è¿‘çš„æ—¥å¿—è¾“å‡º:
   ```bash
   tail -20 logs/vllm_port_1900_*.out
   ```

3. é™ä½å¹¶å‘æ•°:
   ```bash
   --max-concurrency 5
   ```

#### é—®é¢˜ 8: transformers ç‰ˆæœ¬å†²çª

**é”™è¯¯ä¿¡æ¯**:
```
ImportError: cannot import name 'PreTrainedTokenizer' from 'transformers'
AttributeError: module 'transformers' has no attribute '...'
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡æ–°å®‰è£…å…¼å®¹ç‰ˆæœ¬
pip uninstall -y transformers vllm
pip install vllm==0.9.2 "transformers<4.54.0"
```

### è°ƒè¯•æŠ€å·§

#### 1. ä½¿ç”¨ Smoke Test å¿«é€ŸéªŒè¯

```bash
# æ–¹æ³• 1: ä½¿ç”¨ mock åŸŸï¼ˆæ¨èï¼Œåªæœ‰ 9 ä¸ªä»»åŠ¡ï¼‰
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains mock \
  --num-servers 1 \
  --max-concurrency 1 \
  --log-level DEBUG

# æ–¹æ³• 2: ä½¿ç”¨ retail çš„ç¬¬ 1 ä¸ªä»»åŠ¡
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains retail \
  --num-tasks 1 \
  --num-servers 1 \
  --max-concurrency 1 \
  --log-level DEBUG
```

#### 2. æŸ¥çœ‹è¯¦ç»†çš„ LLM è°ƒç”¨

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡å¯ç”¨ trace
export TAU2_TRACE=1

# è¿è¡Œè¯„æµ‹
python run_local.py --agent-model $CKPT_DIR --log-level DEBUG
```

#### 3. åˆ†æ vLLM Server æ€§èƒ½

```bash
# æŸ¥çœ‹ GPU åˆ©ç”¨ç‡
watch -n 1 nvidia-smi

# æŸ¥çœ‹ vLLM è¯·æ±‚ç»Ÿè®¡
curl http://localhost:1900/metrics
```

#### 4. æ£€æŸ¥ç½‘ç»œè¿æ¥

```bash
# æµ‹è¯• server æ˜¯å¦å¯è®¿é—®
curl http://localhost:1900/health

# æŸ¥çœ‹å½“å‰è¿æ¥
netstat -an | grep 1900
```

---

## ğŸ“ ä¸å®˜æ–¹ run.py çš„å¯¹æ¯”

| ç‰¹æ€§ | run.py (SLURM) | run_local.py (æœ¬åœ°) |
|------|----------------|---------------------|
| **è°ƒåº¦ç³»ç»Ÿ** | SLURM é›†ç¾¤ | ç›´æ¥å¯åŠ¨è¿›ç¨‹ |
| **æœåŠ¡å™¨ IP** | é›†ç¾¤èŠ‚ç‚¹ IPï¼ˆåŠ¨æ€ï¼‰ | 127.0.0.1 (localhost) |
| **GPU åˆ†é…** | SLURM è‡ªåŠ¨åˆ†é…ï¼ˆ8 GPUï¼‰ | æ‰‹åŠ¨æŒ‡å®š `--gpu-ids` |
| **è¯„æµ‹åŸŸ** | retail, telecom, airline | **å®Œå…¨ç›¸åŒ** âœ… |
| **ä»»åŠ¡æ–‡ä»¶** | tasks.jsonï¼ˆç›¸åŒæ–‡ä»¶ï¼‰ | **å®Œå…¨ç›¸åŒ** âœ… |
| **Expert Routing** | å¯ç”¨ (`--use_model_tool`) | **é»˜è®¤å¯ç”¨** âœ… |
| **User LLM** | gpt-5 | gpt-5ï¼ˆé»˜è®¤ï¼‰ âœ… |
| **Max Steps** | 200 | 200ï¼ˆé»˜è®¤ï¼‰ âœ… |
| **Judge Model** | å¯åŠ¨æœ¬åœ° Qwen3-32B servers | é€šè¿‡ API è°ƒç”¨ gpt-5 |
| **æ—¥å¿—è¾“å‡º** | SLURM ä½œä¸šè¾“å‡ºæ–‡ä»¶ | stdout + æœ¬åœ° logs/ |
| **è¿›åº¦æ˜¾ç¤º** | æ—  | rich è¿›åº¦æ¡ + ETA âœ¨ |
| **æ—¥å¿—ç³»ç»Ÿ** | ç®€å• print | ç»“æ„åŒ– loggingï¼ˆPROFILE/DEBUGï¼‰âœ¨ |
| **å¾ªç¯è¿è¡Œ** | æ˜¯ï¼ˆæŒç»­ç›‘æ§ä»»åŠ¡é˜Ÿåˆ—ï¼‰ | å¦ï¼ˆè¿è¡Œä¸€æ¬¡åé€€å‡ºï¼‰ |

**âœ… å®Œå…¨å¯¹é½**: é»˜è®¤å‚æ•°ä¸‹ï¼Œ`run_local.py` ä¸å®˜æ–¹ `run.py` çš„è¯„æµ‹é…ç½®**å®Œå…¨ä¸€è‡´**:

```bash
# é»˜è®¤é…ç½®å³ä¸ run.py å¯¹é½
python run_local.py --agent-model $CKPT_DIR
```

**å¦‚æœä¸éœ€è¦ expert routing**ï¼ˆæ›´å¿«æ›´ä¾¿å®œï¼Œä½†åç¦»å®˜æ–¹é…ç½®ï¼‰:
```bash
python run_local.py --agent-model $CKPT_DIR --no-use-model-tool
```

**ä¸å¯ç”¨ expert çš„å½±å“**:
- Agent åªèƒ½ä½¿ç”¨è‡ªå·±çš„èƒ½åŠ›ï¼Œä¸èƒ½è°ƒç”¨ expert æ¨¡å‹
- æ€§èƒ½å¯èƒ½ç•¥ä½äºå®˜æ–¹ run.py çš„ç»“æœ
- ä½†è¯„æµ‹é€Ÿåº¦æ›´å¿«ï¼ˆæ—  expert API è°ƒç”¨ï¼‰ï¼Œæˆæœ¬æ›´ä½ï¼ˆæ—  OpenAI API è´¹ç”¨ï¼‰

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å¹¶è¡ŒæœåŠ¡å™¨**: å¦‚æœæœ‰å¤šä¸ª GPUï¼Œå¯åŠ¨å¤šä¸ªæœåŠ¡å™¨å¯ä»¥åŠ å¿«è¯„æµ‹é€Ÿåº¦
   ```bash
   --num-servers 4  # 4 ä¸ª GPUï¼Œæ¯ä¸ªè¿è¡Œä¸€ä¸ªæœåŠ¡å™¨
   ```

2. **å‡å°‘å¯åŠ¨å»¶è¿Ÿ**: å¦‚æœ GPU å†…å­˜å……è¶³ï¼Œå¯ä»¥å‡å°‘å¯åŠ¨é—´éš”
   ```bash
   --stagger-delay 30  # ä»é»˜è®¤çš„ 60 ç§’å‡å°‘åˆ° 30 ç§’
   ```

3. **åˆ†æ‰¹è¯„æµ‹**: å¦‚æœæ—¶é—´æœ‰é™ï¼Œå¯ä»¥å…ˆè¯„æµ‹å•ä¸ªåŸŸ
   ```bash
   --domains retail  # åªè¯„æµ‹ retailï¼Œä¹‹åå†è¯„æµ‹å…¶ä»–åŸŸ
   ```

4. **è°ƒæ•´å¹¶å‘**: é€šè¿‡å¯åŠ¨å¤šä¸ªæœåŠ¡å™¨å®ä¾‹æ¥æé«˜ååé‡
   - 1 ä¸ª GPU: `--num-servers 1`
   - 4 ä¸ª GPU: `--num-servers 4`
   - 8 ä¸ª GPU: `--num-servers 8`

5. **ä½¿ç”¨åˆé€‚çš„æ—¥å¿—çº§åˆ«**: 
   - æ­£å¸¸è¯„æµ‹: `--log-level INFO`ï¼ˆæ˜¾ç¤ºè¿›åº¦æ¡ï¼Œæ€§èƒ½æœ€ä½³ï¼‰
   - æ€§èƒ½åˆ†æ: `--log-level PROFILE`ï¼ˆè®°å½•è¯¦ç»†è®¡æ—¶ï¼Œç•¥å¾®å½±å“æ€§èƒ½ï¼‰
   - è°ƒè¯•é—®é¢˜: `--log-level DEBUG`ï¼ˆæ˜¾ç¤ºæ‰€æœ‰ç»†èŠ‚ï¼Œä¼šäº§ç”Ÿå¤§é‡è¾“å‡ºï¼‰

## ğŸ”¬ æ€§èƒ½åˆ†æå·¥ä½œæµ

### 1. æ”¶é›† PROFILE æ•°æ®

```bash
# è¿è¡Œè¯„æµ‹å¹¶ä¿å­˜æ—¥å¿—
python run_local.py \
  --agent-model $CKPT_DIR \
  --domains retail \
  --log-level PROFILE \
  2>&1 | tee full_eval.log

# æå– PROFILE æ—¥å¿—
grep "\[PROFILE\]" full_eval.log > profile_data.log
grep "\[USER_JUDGE\]" full_eval.log > judge_data.log
```

### 2. åˆ†æä¸åŒç±»å‹çš„è°ƒç”¨

```bash
# ç»Ÿè®¡å„ç±»å‹ tool call çš„æ•°é‡å’Œæ—¶é—´
for type in llm_call expert_call tool_call step_complete; do
  echo "=== $type ==="
  grep "type=$type" profile_data.log | wc -l
  grep "type=$type" profile_data.log | \
    grep -oP 'duration_ms=\K[0-9.]+' | \
    awk '{sum+=$1; sumsq+=$1*$1; count++} 
         END {mean=sum/count; 
              print "Count:", count, 
                    "Mean:", mean, "ms", 
                    "StdDev:", sqrt(sumsq/count - mean*mean), "ms"}'
done
```

### 3. åˆ†æä¸åŒæ¨¡å‹çš„æ€§èƒ½

```bash
# åˆ†æä¸åŒæ¨¡å‹çš„è°ƒç”¨æ—¶é—´
for model in vllm openai claude; do
  echo "=== call_type=$model ==="
  grep "call_type=$model" profile_data.log | \
    grep -oP 'duration_ms=\K[0-9.]+' | \
    awk '{sum+=$1; count++} END {print "Total:", sum, "ms, Avg:", sum/count, "ms"}'
done
```

### 4. æŒ‰ä»»åŠ¡åˆ†æ

```bash
# æ‰¾å‡ºæœ€æ…¢çš„ä»»åŠ¡
grep "type=step_complete" profile_data.log | \
  awk '{
    for(i=1;i<=NF;i++) {
      if($i~/task=/) task=$i;
      if($i~/duration_ms=/) {
        split($i,a,"="); 
        duration=a[2];
      }
    }
    sum[task]+=duration; count[task]++;
  } 
  END {
    for(t in sum) 
      print t, "Total:", sum[t], "ms, Avg:", sum[t]/count[t], "ms"
  }' | sort -k3 -nr | head -10
```

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹æ£€æŸ¥æ¸…å•

åœ¨è¿è¡Œè¯„æµ‹å‰ï¼Œç¡®ä¿ä»¥ä¸‹æ‰€æœ‰é¡¹ç›®å·²å®Œæˆï¼š

### ç¯å¢ƒå‡†å¤‡

- [ ] **Conda ç¯å¢ƒ**
  ```bash
  conda activate vllm1
  python --version  # åº”è¯¥æ˜¯ Python 3.12
  ```

- [ ] **ä¾èµ–å®‰è£…**
  ```bash
  cd /path/to/ToolOrchestra/evaluation/tau2-bench
  pip install -e .
  pip list | grep vllm  # éªŒè¯ vLLM å·²å®‰è£…
  ```

- [ ] **ç¯å¢ƒå˜é‡**
  ```bash
  source setup_envs.sh
  # éªŒè¯å…³é”®å˜é‡
  echo $CKPT_DIR
  echo $REPO_PATH
  echo $OPENAI_API_KEY | cut -c1-10
  ```

### èµ„æºéªŒè¯

- [ ] **Agent æ¨¡å‹**
  ```bash
  ls -lh $CKPT_DIR
  # åº”è¯¥çœ‹åˆ°: config.json, model.safetensors, tokenizer.json ç­‰
  ```

- [ ] **GPU å¯ç”¨æ€§**
  ```bash
  nvidia-smi
  # è‡³å°‘ 1 ä¸ª GPUï¼Œæœ‰è¶³å¤Ÿçš„å¯ç”¨å†…å­˜ï¼ˆ16GB+ï¼‰
  ```

- [ ] **ä»»åŠ¡æ•°æ®æ–‡ä»¶**
  ```bash
  ls -lh $REPO_PATH/data/tau2/domains/*/tasks.json
  # åº”è¯¥çœ‹åˆ° retail, telecom, airline ç­‰åŸŸçš„ä»»åŠ¡æ–‡ä»¶
  ```

### è¿è¡Œæµ‹è¯•

- [ ] **Smoke Testï¼ˆå¿…éœ€ï¼‰**
  ```bash
  # æ¨èï¼šä½¿ç”¨ mock åŸŸ
  python run_local.py \
    --agent-model $CKPT_DIR \
    --domains mock \
    --num-servers 1 \
    --log-level INFO
  
  # æˆ–è€…ï¼šretail å‰ 5 ä¸ªä»»åŠ¡
  python run_local.py \
    --agent-model $CKPT_DIR \
    --domains retail \
    --num-tasks 5 \
    --num-servers 1 \
    --log-level INFO
  ```

- [ ] **éªŒè¯è¾“å‡º**
  ```bash
  # æ£€æŸ¥ç»“æœæ–‡ä»¶
  cat outputs/retail.json | python -m json.tool | head -20
  
  # æ£€æŸ¥ server æ—¥å¿—
  ls -lh logs/
  ```

### å¼€å§‹å®Œæ•´è¯„æµ‹

å¦‚æœ smoke test æˆåŠŸï¼š

```bash
python run_local.py \
  --agent-model $CKPT_DIR \
  --user-llm gpt-5 \
  --num-servers 4 \
  --domains retail telecom airline \
  --log-level INFO \
  --output-dir outputs/full_eval
```

---

## ğŸŒ æ”¯æŒçš„åŸŸï¼ˆDomainsï¼‰

### å½“å‰å¯è¯„æµ‹çš„åŸŸï¼ˆæœ‰ä»»åŠ¡æ–‡ä»¶ï¼‰

**run_local.py** ç›®å‰æ”¯æŒä»¥ä¸‹ 4 ä¸ªåŸŸï¼š

| åŸŸå | è¯´æ˜ | ä»»åŠ¡æ–‡ä»¶ | ä»»åŠ¡æ•° |
|------|------|----------|--------|
| **mock** | æµ‹è¯•åŸŸï¼ˆç®€å•ä»»åŠ¡ï¼‰ | `mock/tasks.json` | 9 |
| **retail** | é›¶å”®å®¢æœï¼ˆè®¢å•ã€é€€è´§ã€äº§å“æŸ¥è¯¢ï¼‰ | `retail/tasks.json` | 114 |
| **telecom** | ç”µä¿¡å®¢æœï¼ˆå¥—é¤ã€è´¦å•ã€æŠ€æœ¯æ”¯æŒï¼‰ | `telecom/tasks.json` | 114 |
| **airline** | èˆªç©ºå®¢æœï¼ˆè®¢ç¥¨ã€æ”¹ç­¾ã€è¡Œæï¼‰ | `airline/original_tasks.json` | 50 |

**é»˜è®¤è¯„æµ‹**: `retail`, `telecom`, `airline`ï¼ˆ278 ä»»åŠ¡ï¼Œä¸åŒ…æ‹¬ mockï¼‰  
**å…¨éƒ¨è¯„æµ‹**: `mock`, `retail`, `telecom`, `airline`ï¼ˆ287 ä»»åŠ¡ï¼‰

**ä½¿ç”¨æ–¹æ³•**:
```bash
# è¯„æµ‹é»˜è®¤çš„ä¸‰ä¸ªåŸŸ
python run_local.py --agent-model $CKPT_DIR

# è¯„æµ‹å•ä¸ªåŸŸ
python run_local.py --agent-model $CKPT_DIR --domains retail

# è¯„æµ‹å¤šä¸ªåŸŸ
python run_local.py --agent-model $CKPT_DIR --domains retail telecom

# åŒ…å« mock æµ‹è¯•åŸŸ
python run_local.py --agent-model $CKPT_DIR --domains mock retail

# è¯„æµ‹æ‰€æœ‰å¯ç”¨åŸŸ
python run_local.py --agent-model $CKPT_DIR --domains mock retail telecom airline
```

### å…¶ä»–å·²æ³¨å†Œçš„åŸŸï¼ˆæš‚æ— ä»»åŠ¡æ–‡ä»¶ï¼‰

ä»¥ä¸‹åŸŸå·²åœ¨ registry ä¸­æ³¨å†Œï¼Œä½†**å½“å‰æ²¡æœ‰ä»»åŠ¡æ–‡ä»¶**ï¼Œæ— æ³•ç›´æ¥é€šè¿‡ `run_local.py` è¯„æµ‹ï¼š

- **bank**: é“¶è¡ŒæœåŠ¡
- **basketball**: ç¯®çƒèµ›äº‹
- **ecommerce**: ç”µå•†å¹³å°
- **medicine**: åŒ»ç–—å’¨è¯¢
- **movie**: ç”µå½±ç¥¨åŠ¡
- **railway**: é“è·¯ç¥¨åŠ¡
- **restaurant**: é¤å…é¢„è®¢
- **school**: å­¦æ ¡è¡Œæ”¿
- **travel**: æ—…è¡Œé¢„è®¢
- **weather**: å¤©æ°”æŸ¥è¯¢

**è¯´æ˜**: è¿™äº›åŸŸæœ‰ç¯å¢ƒå®šä¹‰ï¼ˆpolicy, database, toolsï¼‰ï¼Œä½†ç¼ºå°‘è¯„æµ‹ä»»åŠ¡ã€‚å¦‚æœä½ éœ€è¦è¯„æµ‹è¿™äº›åŸŸï¼Œéœ€è¦ï¼š
1. åˆ›å»ºä»»åŠ¡æ–‡ä»¶ `data/tau2/domains/{domain}/tasks.json`
2. æ›´æ–° `run_local.py` çš„ `task_paths` å­—å…¸

### Telecom åŸŸçš„å˜ä½“

Telecom åŸŸæœ‰å¤šä¸ªä»»åŠ¡æ–‡ä»¶ï¼š
- `tasks.json`: æ ‡å‡†ä»»åŠ¡é›†ï¼ˆé»˜è®¤ä½¿ç”¨ï¼Œ114 ä»»åŠ¡ï¼‰
- `tasks_small.json`: å°è§„æ¨¡ä»»åŠ¡é›†ï¼ˆ20 ä»»åŠ¡ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
- `tasks_full.json`: å®Œæ•´ä»»åŠ¡é›†ï¼ˆæ ¼å¼æœ‰é—®é¢˜ï¼Œæš‚ä¸å¯ç”¨ï¼‰

**ä½¿ç”¨ tasks_small.json**:
```bash
# æ–¹æ³• 1: ä¸´æ—¶ä¿®æ”¹ run_local.py çš„ task_pathsï¼ˆä¸æ¨èï¼‰

# æ–¹æ³• 2: ç›´æ¥ä½¿ç”¨ tau2.cliï¼ˆæ¨èï¼‰
python -m tau2.cli --domain telecom \
  --agent-llm $CKPT_DIR \
  --user-llm gpt-5 \
  --task_path $REPO_PATH/data/tau2/domains/telecom/tasks_small.json \
  --output_file outputs/telecom_small.json \
  --model_config_path model_config_local.json \
  --max-concurrency 10 \
  --log-level INFO

# æ³¨æ„ï¼šéœ€è¦å…ˆå¯åŠ¨ vLLM servers å¹¶åˆ›å»º model_config_local.json
```

### æŸ¥çœ‹ä»»åŠ¡æ–‡ä»¶å†…å®¹

æŸ¥çœ‹å„åŸŸæœ‰å¤šå°‘ä»»åŠ¡ï¼š

```bash
# æŸ¥çœ‹ä»»åŠ¡æ•°é‡
for domain in mock retail telecom airline; do
  file="$REPO_PATH/data/tau2/domains/$domain/tasks.json"
  if [ "$domain" = "airline" ]; then
    file="$REPO_PATH/data/tau2/domains/airline/original_tasks.json"
  fi
  count=$(python -c "import json; print(len(json.load(open('$file'))))")
  echo "$domain: $count tasks"
done

# æŸ¥çœ‹å•ä¸ªä»»åŠ¡çš„ç»“æ„
python -c "
import json
with open('$REPO_PATH/data/tau2/domains/retail/tasks.json') as f:
    tasks = json.load(f)
    print(json.dumps(tasks[0], indent=2))
" | head -50
```

---

## ğŸ“ è·å–å¸®åŠ©

### å‘½ä»¤è¡Œå¸®åŠ©

æŸ¥çœ‹æ‰€æœ‰å¯ç”¨é€‰é¡¹å’Œè¯´æ˜ï¼š
```bash
python run_local.py --help
```

### è¯Šæ–­è„šæœ¬

å¿«é€Ÿè¯Šæ–­ç¯å¢ƒé…ç½®ï¼š
```bash
# æ£€æŸ¥ç¯å¢ƒ
echo "=== Environment ==="
echo "CKPT_DIR: $CKPT_DIR"
echo "REPO_PATH: $REPO_PATH"
echo "Python: $(python --version)"
echo "Conda env: $CONDA_DEFAULT_ENV"

# æ£€æŸ¥ GPU
echo -e "\n=== GPUs ==="
nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv

# æ£€æŸ¥ä¾èµ–
echo -e "\n=== Dependencies ==="
pip list | grep -E "vllm|torch|transformers|rich"

# æ£€æŸ¥ç«¯å£
echo -e "\n=== Ports ==="
netstat -tuln | grep -E "1900|1901|1902|1903"
```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯
grep -i error logs/*.err | tail -20

# æŸ¥çœ‹ vLLM å¯åŠ¨æ—¥å¿—
tail -100 logs/vllm_port_1900_*.out

# ç»Ÿè®¡ä»»åŠ¡æˆåŠŸç‡
python -c "
import json
with open('outputs/retail.json') as f:
    data = json.load(f)
total = len(data['simulations'])
success = sum(1 for s in data['simulations'] if s['reward_info']['reward'] > 0)
print(f'Success rate: {success}/{total} ({100*success/total:.1f}%)')
"
```

### ç¤¾åŒºèµ„æº

- **GitHub Issues**: [nvidia/ToolOrchestra/issues](https://github.com/nvidia/ToolOrchestra/issues)
- **æ–‡æ¡£**: `evaluation/tau2-bench/README.md`
- **åŸç‰ˆ tau2-bench**: [sierra-research/tau-bench](https://github.com/sierra-research/tau-bench)

---

## ğŸ“ æ€»ç»“

### åŸºæœ¬å·¥ä½œæµç¨‹

```mermaid
graph TD
    A[å‡†å¤‡ç¯å¢ƒ] -->|conda + pip| B[é…ç½®å˜é‡]
    B -->|setup_envs.sh| C[ä¸‹è½½æ¨¡å‹]
    C -->|huggingface-cli| D[Smoke Test]
    D -->|--num-tasks 5| E{æµ‹è¯•é€šè¿‡?}
    E -->|No| F[æ•…éšœæ’æŸ¥]
    F --> D
    E -->|Yes| G[å®Œæ•´è¯„æµ‹]
    G -->|run_local.py| H[åˆ†æç»“æœ]
    H --> I[æ€§èƒ½ä¼˜åŒ–]
```

### å…³é”®å‘½ä»¤é€ŸæŸ¥

| ç›®çš„ | å‘½ä»¤ |
|------|------|
| å¿«é€Ÿæµ‹è¯• | `python run_local.py --agent-model $CKPT_DIR --domains mock --no-use-model-tool` |
| æ ‡å‡†è¯„æµ‹ï¼ˆå¸¦ expertï¼‰ | `python run_local.py --agent-model $CKPT_DIR --log-level INFO` |
| å•åŸŸè¯„æµ‹ï¼ˆå¸¦ expertï¼‰ | `python run_local.py --agent-model $CKPT_DIR --domains retail` |
| æ€§èƒ½åˆ†æï¼ˆæŸ¥çœ‹ expertï¼‰ | `python run_local.py --agent-model $CKPT_DIR --domains retail --log-level PROFILE 2>&1 \| tee eval.log` |
| ä¸å¸¦ expertï¼ˆå¿«é€Ÿï¼‰ | `python run_local.py --agent-model $CKPT_DIR --domains retail --no-use-model-tool` |
| è°ƒè¯•æ¨¡å¼ | `python run_local.py --agent-model $CKPT_DIR --domains mock --log-level DEBUG --no-use-model-tool` |
| æŸ¥çœ‹å¸®åŠ© | `python run_local.py --help` |

### æœ€ä½³å®è·µ

1. **æ¸è¿›å¼æµ‹è¯•**: Smoke test â†’ å•åŸŸ â†’ å¤šåŸŸ
2. **åˆç†é…ç½®**: æ ¹æ® GPU æ•°é‡è°ƒæ•´ `--num-servers` å’Œ `--max-concurrency`
3. **æ—¥å¿—ç®¡ç†**: ä½¿ç”¨ `tee` ä¿å­˜æ—¥å¿—ï¼Œä¾¿äºåç»­åˆ†æ
4. **æ€§èƒ½ç›‘æ§**: è¿è¡Œæ—¶ç”¨ `watch nvidia-smi` ç›‘æ§ GPU ä½¿ç”¨
5. **å¤‡ä»½ç»“æœ**: å®šæœŸå¤‡ä»½ `outputs/` ç›®å½•
6. **å¢é‡è¯„æµ‹**: å¤§è§„æ¨¡è¯„æµ‹æ—¶åˆ†åŸŸè¿è¡Œï¼Œé¿å…ä¸€æ¬¡æ€§å¤±è´¥

### å¸¸ç”¨å‚æ•°ç»„åˆ

```bash
# å¼€å‘è°ƒè¯•ï¼ˆmock åŸŸï¼Œç¦ç”¨ expertï¼‰
--domains mock --num-servers 1 --max-concurrency 1 --log-level DEBUG --no-use-model-tool

# å¿«é€ŸéªŒè¯ï¼ˆå•åŸŸå‰ 50 ä¸ªä»»åŠ¡ï¼Œå¸¦ expertï¼‰
--domains retail --num-tasks 50 --num-servers 2 --max-concurrency 5 --log-level INFO

# æ ‡å‡†è¯„æµ‹ï¼ˆä¸‰ä¸ªä¸»è¦åŸŸï¼Œ278 ä»»åŠ¡ï¼Œå¸¦ expertï¼Œé»˜è®¤é…ç½®ï¼‰
--domains retail telecom airline --num-servers 4 --max-concurrency 10 --log-level INFO

# æ€§èƒ½åˆ†æï¼ˆå•åŸŸï¼ŒæŸ¥çœ‹ expert è°ƒç”¨è¯¦æƒ…ï¼‰
--domains retail --num-servers 4 --max-concurrency 10 --log-level PROFILE

# ä¸å¸¦ expert çš„å¿«é€Ÿè¯„æµ‹ï¼ˆçœæ—¶çœé’±ï¼‰
--domains retail --num-servers 2 --max-concurrency 5 --no-use-model-tool --log-level INFO

# é«˜ååé‡ï¼ˆæ‰€æœ‰åŸŸï¼Œ287 ä»»åŠ¡ï¼Œå¸¦ expertï¼‰
--domains mock retail telecom airline --num-servers 8 --max-concurrency 24 --stagger-delay 30
```

**ğŸ’¡ æ³¨æ„**: é™¤éæ˜ç¡®æŒ‡å®š `--no-use-model-tool`ï¼Œå¦åˆ™ expert routing é»˜è®¤å¯ç”¨ã€‚

---

## ğŸ™ è‡´è°¢

æœ¬æŒ‡å—åŸºäº NVIDIA ToolOrchestra é¡¹ç›®å’Œ Sierra Research çš„ tau-benchã€‚æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…ï¼
