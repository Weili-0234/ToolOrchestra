# WORKLOG_20251231

## Summary

Goal: accelerate FRAMES `search` tool E2E latency by fixing the wiki retrieval server bottleneck (HF datasets random I/O) and validating via stress tests + FRAMES first-10 tasks.

## Key Code Changes

### 1) Wiki retrieval server speedups
- **File**: `evaluation/retrieval_wiki.py`
- **Implemented**:
  - **Corpus preloading into RAM** (`WIKI_PRELOAD_CORPUS=1`): avoids HF-datasets random access per-doc disk I/O; `load_docs()` becomes list indexing.
  - **Server-side request batching** (`WIKI_REQUEST_BATCHING=1`): batches concurrent `/retrieve` calls to amortize Qwen3 embedding encoding.
  - **GPU FAISS sharding** (`WIKI_FAISS_GPU=1`): FAISS index moved to GPU(s) with sharding + FP16 via `GpuMultipleClonerOptions`.
  - **Avoid per-request `torch.cuda.empty_cache()`** (default off via `WIKI_TORCH_EMPTY_CACHE=0`): reduces latency spikes.

### 2) FRAMES latency/profiling data fix
- **File**: `evaluation/eval_frames_oss.py`
- **Implemented**:
  - Persist `all_tool_responses` in each task JSON output (unblocks `analyze_tool_latency.py` for FRAMES).
  - Add timing fields:
    - `search`: `_query_llm_ms`, `_retrieval_ms`, `_retrieval_retries`
    - `answer`: `_expert_ms`, `_judge_ms` (if judge used)
    - `enhance_reasoning`: `_llm_ms`, `_exec_ms`

### 3) SLURM deployment + validation utilities
- **File**: `evaluation/oss_scripts/slurm_launch_wiki_retrieval_fast.sh`
  - SLURM wrapper to launch `launch_wiki_retrieval.sh` with:
    - corpus preload + batching enabled
    - optional GPU FAISS enabled (latest version: 3 GPUs)
    - **auto-pick free GPUs** on the node (handles “dirty GPU0” nodes)
- **File**: `evaluation/stress_test_wiki_retrieval.py`
  - Simple concurrent `/retrieve` stress tester with p50/p80/p95 stats.
- **Files**:
  - `evaluation/model_config_oss_new_wiki_fast19.json` (FRAMES uses `wiki_retrieval` at `research-secure-19:8766`)
  - `evaluation/model_config_oss_new_wiki_fast17.json` (earlier run; superseded)

## SLURM Deployment / Debugging Notes

### Observed issues
- **Dirty GPU nodes**:
  - Found a node with a large unrelated process on GPU0 (~63GB) causing OOM when loading Qwen3 embedding model.
  - Implemented “auto-pick free GPU(s)” in `slurm_launch_wiki_retrieval_fast.sh` to avoid this class of failure.
- **CPU FAISS is too slow for target**:
  - Even after corpus preload, CPU FAISS + encoding led to high p50/p80 under concurrency.
  - Switching to **GPU FAISS sharding** is the critical step to meet <5s P80.

### Job IDs / Nodes (for traceability)
- `13494` (research-secure-17): corpus preload worked; CPU FAISS path still too slow under concurrency.
- `13497` (research-secure-18): failed due to dirty GPU (unrelated large process).
- `13500` (research-secure-19): fast-fail added (detected dirty GPU0).
- `13501` (research-secure-19): CPU FAISS + preload ran, but retrieval latency still high.
- `13502` (research-secure-19): **GPU FAISS + preload + batching** (final validated configuration).

## Results

### 1) Retrieval server stress test (final: GPU FAISS + batching)
Service: `http://research-secure-19:8766` (job `13502`)

- **Single concurrency (20 req)**:
  - p50 ~ **0.120s**, p80 ~ **0.122s**
- **32 concurrency (256 req)**:
  - p50 ~ **0.304s**, p80 ~ **0.333s**, p95 ~ **0.362s**
  - throughput ~ **~100 rps**

Conclusion: **retrieval server is no longer the latency bottleneck**.

### 2) FRAMES first 10 tasks (smoke)
Command (run under `conda activate toolorchestra`):
- Output dir: `evaluation/outputs_oss_frames_smoke_wiki_fast19_20251231_175301`
- Config: `evaluation/model_config_oss_new_wiki_fast19.json` (wiki retriever -> `research-secure-19:8766`)

`python analyze_tool_latency.py outputs_oss_frames_smoke_wiki_fast19_20251231_175301`

- **search**:
  - p50 ~ **4.92s**
  - p90 ~ **42.46s**
  - <5s% ~ **55.6%**
  - Observation: `search expert` ≈ `search e2e`, implying the **query LLM (gpt-oss-20b)** tail dominates now.

Conclusion: Retrieval is fixed; remaining work is to reduce **query generation tail latency**.

## Next Steps / Recommendations

1. **Fix search-query LLM tail latency**:
   - Increase/scale out `gpt-oss-20b` endpoints (DP), or move to a more stable/low-tail model for query writing.
   - Add request-level timeouts and fallback to simpler query generation on slow responses.
2. Consider enabling **multi-instance retriever** (two ports) only if throughput becomes a bottleneck; current retriever throughput is already high.


